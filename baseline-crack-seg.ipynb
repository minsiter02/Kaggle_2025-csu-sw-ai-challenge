{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-13T11:26:07.826822Z",
     "iopub.status.busy": "2025-10-13T11:26:07.826549Z",
     "iopub.status.idle": "2025-10-13T11:27:16.075877Z",
     "shell.execute_reply": "2025-10-13T11:27:16.074947Z",
     "shell.execute_reply.started": "2025-10-13T11:26:07.826801Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install -q ptflops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /home/jms/.local/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (2.2.6)\n",
      "Requirement already satisfied: Pillow in /home/jms/.local/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (11.3.0)\n",
      "Requirement already satisfied: scikit-learn in /home/jms/.local/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (1.7.2)\n",
      "Requirement already satisfied: torch in /home/jms/.local/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (2.8.0)\n",
      "Requirement already satisfied: torchvision in /home/jms/.local/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (0.23.0)\n",
      "Requirement already satisfied: ptflops in /home/jms/.local/lib/python3.12/site-packages (from -r requirements.txt (line 6)) (0.7.5)\n",
      "Requirement already satisfied: pandas in /home/jms/.local/lib/python3.12/site-packages (from -r requirements.txt (line 7)) (2.3.3)\n",
      "Requirement already satisfied: segmentation-models-pytorch in /home/jms/.local/lib/python3.12/site-packages (from -r requirements.txt (line 8)) (0.5.0)\n",
      "Requirement already satisfied: albumentations in /home/jms/.local/lib/python3.12/site-packages (from -r requirements.txt (line 9)) (2.0.8)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/jms/.local/lib/python3.12/site-packages (from scikit-learn->-r requirements.txt (line 3)) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/jms/.local/lib/python3.12/site-packages (from scikit-learn->-r requirements.txt (line 3)) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/jms/.local/lib/python3.12/site-packages (from scikit-learn->-r requirements.txt (line 3)) (3.6.0)\n",
      "Requirement already satisfied: filelock in /home/jms/.local/lib/python3.12/site-packages (from torch->-r requirements.txt (line 4)) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/jms/.local/lib/python3.12/site-packages (from torch->-r requirements.txt (line 4)) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from torch->-r requirements.txt (line 4)) (68.1.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/jms/.local/lib/python3.12/site-packages (from torch->-r requirements.txt (line 4)) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/jms/.local/lib/python3.12/site-packages (from torch->-r requirements.txt (line 4)) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch->-r requirements.txt (line 4)) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/jms/.local/lib/python3.12/site-packages (from torch->-r requirements.txt (line 4)) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/jms/.local/lib/python3.12/site-packages (from torch->-r requirements.txt (line 4)) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/jms/.local/lib/python3.12/site-packages (from torch->-r requirements.txt (line 4)) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/jms/.local/lib/python3.12/site-packages (from torch->-r requirements.txt (line 4)) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/jms/.local/lib/python3.12/site-packages (from torch->-r requirements.txt (line 4)) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/jms/.local/lib/python3.12/site-packages (from torch->-r requirements.txt (line 4)) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/jms/.local/lib/python3.12/site-packages (from torch->-r requirements.txt (line 4)) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/jms/.local/lib/python3.12/site-packages (from torch->-r requirements.txt (line 4)) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/jms/.local/lib/python3.12/site-packages (from torch->-r requirements.txt (line 4)) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/jms/.local/lib/python3.12/site-packages (from torch->-r requirements.txt (line 4)) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/jms/.local/lib/python3.12/site-packages (from torch->-r requirements.txt (line 4)) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/jms/.local/lib/python3.12/site-packages (from torch->-r requirements.txt (line 4)) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/jms/.local/lib/python3.12/site-packages (from torch->-r requirements.txt (line 4)) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/jms/.local/lib/python3.12/site-packages (from torch->-r requirements.txt (line 4)) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/jms/.local/lib/python3.12/site-packages (from torch->-r requirements.txt (line 4)) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /home/jms/.local/lib/python3.12/site-packages (from torch->-r requirements.txt (line 4)) (3.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/jms/.local/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 7)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->-r requirements.txt (line 7)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/jms/.local/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 7)) (2025.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.24 in /home/jms/.local/lib/python3.12/site-packages (from segmentation-models-pytorch->-r requirements.txt (line 8)) (0.35.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/jms/.local/lib/python3.12/site-packages (from segmentation-models-pytorch->-r requirements.txt (line 8)) (0.6.2)\n",
      "Requirement already satisfied: timm>=0.9 in /home/jms/.local/lib/python3.12/site-packages (from segmentation-models-pytorch->-r requirements.txt (line 8)) (1.0.20)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/jms/.local/lib/python3.12/site-packages (from segmentation-models-pytorch->-r requirements.txt (line 8)) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in /usr/lib/python3/dist-packages (from albumentations->-r requirements.txt (line 9)) (6.0.1)\n",
      "Requirement already satisfied: pydantic>=2.9.2 in /home/jms/.local/lib/python3.12/site-packages (from albumentations->-r requirements.txt (line 9)) (2.12.2)\n",
      "Requirement already satisfied: albucore==0.0.24 in /home/jms/.local/lib/python3.12/site-packages (from albumentations->-r requirements.txt (line 9)) (0.0.24)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /home/jms/.local/lib/python3.12/site-packages (from albumentations->-r requirements.txt (line 9)) (4.12.0.88)\n",
      "Requirement already satisfied: stringzilla>=3.10.4 in /home/jms/.local/lib/python3.12/site-packages (from albucore==0.0.24->albumentations->-r requirements.txt (line 9)) (4.2.1)\n",
      "Requirement already satisfied: simsimd>=5.9.2 in /home/jms/.local/lib/python3.12/site-packages (from albucore==0.0.24->albumentations->-r requirements.txt (line 9)) (6.5.3)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/jms/.local/lib/python3.12/site-packages (from huggingface-hub>=0.24->segmentation-models-pytorch->-r requirements.txt (line 8)) (25.0)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch->-r requirements.txt (line 8)) (2.31.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/jms/.local/lib/python3.12/site-packages (from huggingface-hub>=0.24->segmentation-models-pytorch->-r requirements.txt (line 8)) (1.1.10)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/jms/.local/lib/python3.12/site-packages (from pydantic>=2.9.2->albumentations->-r requirements.txt (line 9)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /home/jms/.local/lib/python3.12/site-packages (from pydantic>=2.9.2->albumentations->-r requirements.txt (line 9)) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /home/jms/.local/lib/python3.12/site-packages (from pydantic>=2.9.2->albumentations->-r requirements.txt (line 9)) (0.4.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 7)) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/jms/.local/lib/python3.12/site-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 4)) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T11:27:16.077527Z",
     "iopub.status.busy": "2025-10-13T11:27:16.077295Z",
     "iopub.status.idle": "2025-10-13T11:27:22.548576Z",
     "shell.execute_reply": "2025-10-13T11:27:22.548015Z",
     "shell.execute_reply.started": "2025-10-13T11:27:16.077501Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from sklearn.metrics import jaccard_score\n",
    "import numpy as np\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import time\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import datetime\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 기본 디렉토리 설정 \n",
    "# TRAIN_DIR = \"/kaggle/input/2025-sw-ai/archive/train\"\n",
    "# VAL_DIR = \"/kaggle/input/2025-sw-ai/archive/val\"\n",
    "# TEST_DIR = \"/kaggle/input/2025-sw-ai/archive/test/images\"\n",
    "# OUTPUT_PATH = \"/kaggle/working/submission.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로컬 디렉토리 설정\n",
    "TRAIN_DIR = \"input/2025-csu-sw-ai-challenge/archive/train\" \n",
    "VAL_DIR = \"input/2025-csu-sw-ai-challenge/archive/val\"\n",
    "TEST_DIR = \"input/2025-csu-sw-ai-challenge/archive/test/images\"\n",
    "OUTPUT_CSV = \"working/submission.csv\" \n",
    "OUTPUT_MASK = \"working/mask_ouputs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set SEED: 2025\n"
     ]
    }
   ],
   "source": [
    "SEED = 2025\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f'set SEED: {SEED}')\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T11:27:22.549403Z",
     "iopub.status.busy": "2025-10-13T11:27:22.549145Z",
     "iopub.status.idle": "2025-10-13T11:27:22.555503Z",
     "shell.execute_reply": "2025-10-13T11:27:22.554811Z",
     "shell.execute_reply.started": "2025-10-13T11:27:22.549387Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jms/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from albumentations import (\n",
    "    Compose, HorizontalFlip, VerticalFlip, RandomRotate90, ShiftScaleRotate,\n",
    "    RandomBrightnessContrast, GaussNoise, OneOf, Blur, MotionBlur, RandomGamma\n",
    ")\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "def get_transform():\n",
    "    return Compose([\n",
    "        HorizontalFlip(p=0.5),\n",
    "        VerticalFlip(p=0.5),\n",
    "        RandomRotate90(p=0.5),\n",
    "        ShiftScaleRotate(shift_limit=0.06, scale_limit=0.15, rotate_limit=45, p=0.5),\n",
    "        OneOf([\n",
    "            GaussNoise(variance=(10.0, 40.0)),\n",
    "            Blur(blur_limit=3),\n",
    "            MotionBlur(blur_limit=3)\n",
    "        ], p=0.5),\n",
    "        RandomBrightnessContrast(p=0.4),\n",
    "        RandomGamma(p=0.3),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "class CrackDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, augment_ratio=1):\n",
    "        self.img_dir = os.path.join(root_dir, \"images\")\n",
    "        self.mask_dir = os.path.join(root_dir, \"masks\")\n",
    "        self.img_list = sorted(glob.glob(self.img_dir + \"/*.jpg\"))\n",
    "        self.mask_list = sorted(glob.glob(self.mask_dir + \"/*.jpg\"))\n",
    "        self.transform = transform\n",
    "        self.augment_ratio = augment_ratio  # 추가!\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list) * self.augment_ratio  # 원본x배수\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        orig_idx = idx // self.augment_ratio  # 원본 인덱스 재설정\n",
    "        img = Image.open(self.img_list[orig_idx]).convert(\"L\")\n",
    "        mask = Image.open(self.mask_list[orig_idx]).convert(\"L\")\n",
    "\n",
    "        img = np.array(img, dtype=np.float32) / 255.0\n",
    "        mask = np.array(mask, dtype=np.float32) / 255.0\n",
    "        mask = (mask > 0.5).astype(np.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=img, mask=mask)\n",
    "            img = augmented['image']  # (1,H,W) tensor\n",
    "            mask = augmented['mask'].unsqueeze(0).float()\n",
    "        else:\n",
    "            img = torch.tensor(img).unsqueeze(0)\n",
    "            mask = torch.tensor(mask).unsqueeze(0)\n",
    "\n",
    "        return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T11:27:22.557199Z",
     "iopub.status.busy": "2025-10-13T11:27:22.556966Z",
     "iopub.status.idle": "2025-10-13T11:27:22.580819Z",
     "shell.execute_reply": "2025-10-13T11:27:22.580155Z",
     "shell.execute_reply.started": "2025-10-13T11:27:22.557184Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "class EfficientCrackNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels=1,\n",
    "                 base=16,\n",
    "                 num_classes=1,\n",
    "                 pretrained=None):\n",
    "        super(EfficientCrackNet, self).__init__()\n",
    "        self.base = base\n",
    "        self.num_classes = num_classes\n",
    "        self.pretrained = pretrained\n",
    "        \n",
    "        # Edge Extraction Method (EEM)\n",
    "        self.eem = EdgeExtractionMethod(in_channels=in_channels, out_channels=base)\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder1 = EncoderBlock(in_channels=base, out_channels=base*2, \n",
    "                                     use_ulsam=True, num_subspaces=4)\n",
    "        self.encoder2 = EncoderBlock(in_channels=base*2, out_channels=base*4, \n",
    "                                     use_ulsam=True, num_subspaces=4)\n",
    "        self.encoder3 = EncoderBlock(in_channels=base*4, out_channels=base*8, \n",
    "                                     use_ulsam=True, num_subspaces=4)\n",
    "        \n",
    "        # Bottleneck with MobileViT\n",
    "        self.bottleneck = MobileViTBlock(\n",
    "            in_channels=base*8,\n",
    "            out_channels=base*8,\n",
    "            transformer_dim=base*4,\n",
    "            num_heads=4,\n",
    "            num_transformer_blocks=2,\n",
    "            patch_size=(2, 2)\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder3 = DecoderBlock(in_channels=base*8, skip_channels=base*8, \n",
    "                                     out_channels=base*4)\n",
    "        self.decoder2 = DecoderBlock(in_channels=base*4, skip_channels=base*4, \n",
    "                                     out_channels=base*2)\n",
    "        self.decoder1 = DecoderBlock(in_channels=base*2, skip_channels=base*2, \n",
    "                                     out_channels=base)\n",
    "        \n",
    "        # Segmentation heads\n",
    "        self.aux_head1 = SegHead(inplanes=base*2, interplanes=base, \n",
    "                                outplanes=num_classes, aux_head=True)\n",
    "        self.aux_head2 = SegHead(inplanes=base*4, interplanes=base*2, \n",
    "                                outplanes=num_classes, aux_head=True)\n",
    "        self.head = SegHead(inplanes=base, interplanes=base, \n",
    "                           outplanes=num_classes, aux_head=False)\n",
    "        \n",
    "        self.init_weight()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h, w = x.shape[2:]\n",
    "        logit_list = []\n",
    "        \n",
    "        # Edge Extraction\n",
    "        eem_out = self.eem(x)\n",
    "        \n",
    "        # Encoder path\n",
    "        enc1 = self.encoder1(eem_out)\n",
    "        enc2 = self.encoder2(enc1)\n",
    "        enc3 = self.encoder3(enc2)\n",
    "        \n",
    "        # Bottleneck\n",
    "        bottle = self.bottleneck(enc3)\n",
    "        \n",
    "        # Decoder path\n",
    "        dec3 = self.decoder3(bottle, enc3)\n",
    "        dec2 = self.decoder2(dec3, enc2)\n",
    "        dec1 = self.decoder1(dec2, enc1)\n",
    "        \n",
    "        # Segmentation heads\n",
    "        if self.training:\n",
    "            main_out = self.head(dec1)\n",
    "            aux_out1 = self.aux_head1(dec2)\n",
    "            aux_out2 = self.aux_head2(dec3)\n",
    "            \n",
    "            logit_list = [main_out, aux_out1, aux_out2]\n",
    "            logit_list = [F.interpolate(logit, size=(h, w), mode='bilinear', \n",
    "                                       align_corners=True) for logit in logit_list]\n",
    "            return logit_list\n",
    "        else:\n",
    "            main_out = self.head(dec1)\n",
    "            logit_list = [main_out]\n",
    "            logit_list = [F.interpolate(logit, size=(h, w), mode='bilinear', \n",
    "                                       align_corners=True) for logit in logit_list]\n",
    "            return logit_list\n",
    "    \n",
    "    def init_weight(self):\n",
    "        if self.pretrained is not None:\n",
    "            pass\n",
    "        else:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "                elif isinstance(m, nn.BatchNorm2d):\n",
    "                    nn.init.constant_(m.weight, 1)\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "# Edge Extraction Method (EEM) - DoG and LoG based\n",
    "class EdgeExtractionMethod(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=16):\n",
    "        super(EdgeExtractionMethod, self).__init__()\n",
    "        \n",
    "        # Gaussian blur for smoothing\n",
    "        self.gaussian = nn.Conv2d(in_channels, in_channels, kernel_size=5, \n",
    "                                  stride=1, padding=2, groups=in_channels, bias=False)\n",
    "        \n",
    "        # Initialize Gaussian kernel\n",
    "        self._init_gaussian_kernel()\n",
    "        \n",
    "        # DoG (Difference of Gaussian) approximation\n",
    "        self.dog_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels//2, kernel_size=3, stride=1, \n",
    "                     padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels//2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # LoG (Laplacian of Gaussian) approximation\n",
    "        self.log_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels//2, kernel_size=3, stride=1, \n",
    "                     padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels//2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Feature refinement\n",
    "        self.refine = nn.Sequential(\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, \n",
    "                     padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, \n",
    "                     padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def _init_gaussian_kernel(self):\n",
    "        # Create 5x5 Gaussian kernel\n",
    "        kernel_size = 5\n",
    "        sigma = 1.0\n",
    "        kernel = torch.zeros((kernel_size, kernel_size))\n",
    "        center = kernel_size // 2\n",
    "        \n",
    "        for i in range(kernel_size):\n",
    "            for j in range(kernel_size):\n",
    "                x, y = i - center, j - center\n",
    "                kernel[i, j] = math.exp(-(x**2 + y**2) / (2 * sigma**2))\n",
    "        \n",
    "        kernel = kernel / kernel.sum()\n",
    "        kernel = kernel.view(1, 1, kernel_size, kernel_size)\n",
    "        kernel = kernel.repeat(self.gaussian.in_channels, 1, 1, 1)\n",
    "        \n",
    "        self.gaussian.weight.data = kernel\n",
    "        self.gaussian.weight.requires_grad = False\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply Gaussian blur\n",
    "        smoothed = self.gaussian(x)\n",
    "        \n",
    "        # Extract edge features using DoG and LoG\n",
    "        dog_features = self.dog_conv(smoothed)\n",
    "        log_features = self.log_conv(smoothed)\n",
    "        \n",
    "        # Concatenate edge features\n",
    "        edge_features = torch.cat([dog_features, log_features], dim=1)\n",
    "        \n",
    "        # Refine features\n",
    "        out = self.refine(edge_features)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "# Ultra-Lightweight Subspace Attention Module (ULSAM)\n",
    "class ULSAM(nn.Module):\n",
    "    def __init__(self, channels, num_subspaces=4):\n",
    "        super(ULSAM, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.num_subspaces = num_subspaces\n",
    "        \n",
    "        assert channels % num_subspaces == 0, \"Channels must be divisible by num_subspaces\"\n",
    "        \n",
    "        self.subspace_channels = channels // num_subspaces\n",
    "        self.subspaces = nn.ModuleList([\n",
    "            SubspaceAttention(self.subspace_channels) for _ in range(num_subspaces)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Split input into subspaces\n",
    "        subspace_features = torch.chunk(x, self.num_subspaces, dim=1)\n",
    "        \n",
    "        # Apply attention to each subspace\n",
    "        attended_features = []\n",
    "        for idx, subspace_feat in enumerate(subspace_features):\n",
    "            attended = self.subspaces[idx](subspace_feat)\n",
    "            attended_features.append(attended)\n",
    "        \n",
    "        # Concatenate attended subspaces\n",
    "        out = torch.cat(attended_features, dim=1)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class SubspaceAttention(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(SubspaceAttention, self).__init__()\n",
    "        \n",
    "        # Depthwise convolution for spatial attention\n",
    "        self.dw_conv = nn.Conv2d(channels, channels, kernel_size=3, stride=1, \n",
    "                                 padding=1, groups=channels, bias=False)\n",
    "        self.bn_dw = nn.BatchNorm2d(channels)\n",
    "        self.relu_dw = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Max pooling for spatial aggregation\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Pointwise convolution to generate attention map\n",
    "        self.pw_conv = nn.Conv2d(channels, 1, kernel_size=1, stride=1, \n",
    "                                padding=0, bias=False)\n",
    "        self.bn_pw = nn.BatchNorm2d(1)\n",
    "        self.relu_pw = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Depthwise convolution\n",
    "        out = self.dw_conv(x)\n",
    "        out = self.bn_dw(out)\n",
    "        out = self.relu_dw(out)\n",
    "        \n",
    "        # Spatial aggregation\n",
    "        out = self.maxpool(out)\n",
    "        \n",
    "        # Generate attention map\n",
    "        out = self.pw_conv(out)\n",
    "        out = self.bn_pw(out)\n",
    "        out = self.relu_pw(out)\n",
    "        \n",
    "        # Apply sigmoid and expand to match input channels\n",
    "        attention_map = self.sigmoid(out)\n",
    "        attention_map = attention_map.expand_as(x)\n",
    "        \n",
    "        # Apply attention and add residual\n",
    "        out = x * attention_map + x\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "# Depthwise Separable Convolution\n",
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(DepthwiseSeparableConv, self).__init__()\n",
    "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size,\n",
    "                                   stride=stride, padding=padding, groups=in_channels, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1,\n",
    "                                   stride=1, padding=0, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.depthwise(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.pointwise(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "# MobileViT Block\n",
    "class MobileViTBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, transformer_dim,\n",
    "                 num_heads=4, num_transformer_blocks=2, patch_size=(2, 2)):\n",
    "        super(MobileViTBlock, self).__init__()\n",
    "        self.patch_h, self.patch_w = patch_size\n",
    "        self.transformer_dim = transformer_dim\n",
    "\n",
    "        # Local feature extractor\n",
    "        self.local_rep = nn.Sequential(\n",
    "            DepthwiseSeparableConv(in_channels, in_channels, kernel_size=3, stride=1, padding=1),\n",
    "            DepthwiseSeparableConv(in_channels, transformer_dim, kernel_size=1, stride=1, padding=0)\n",
    "        )\n",
    "\n",
    "        # Linear projection for transformer\n",
    "        self.fc1 = nn.Linear(self.patch_h * self.patch_w * transformer_dim, transformer_dim)\n",
    "        self.fc2 = nn.Linear(transformer_dim, self.patch_h * self.patch_w * transformer_dim)\n",
    "\n",
    "        # Transformer blocks\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(transformer_dim, num_heads)\n",
    "            for _ in range(num_transformer_blocks)\n",
    "        ])\n",
    "\n",
    "        # Normalization and fusion\n",
    "        self.norm = nn.LayerNorm(transformer_dim)\n",
    "        self.fusion = nn.Sequential(\n",
    "            DepthwiseSeparableConv(transformer_dim, in_channels, kernel_size=1, stride=1, padding=0),\n",
    "            DepthwiseSeparableConv(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "\n",
    "        # Unfold/Fold layers for patch manipulation\n",
    "        self.unfold = nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "        self.fold = nn.Fold(output_size=None, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        local_features = self.local_rep(x)  # (B, transformer_dim, H, W)\n",
    "\n",
    "        # Patchify\n",
    "        patches = self.unfold(local_features)  # (B, patch_dim, num_patches)\n",
    "        patches = patches.transpose(1, 2)      # (B, num_patches, patch_dim)\n",
    "\n",
    "        # Calculate patch dimension\n",
    "        patch_dim = patches.shape[-1]\n",
    "        # Project to transformer dimension\n",
    "        patches = self.fc1(patches)\n",
    "\n",
    "        # Transformer processing\n",
    "        for blk in self.transformer_blocks:\n",
    "            patches = blk(patches)\n",
    "        patches = self.norm(patches)\n",
    "\n",
    "        # Project back\n",
    "        patches = self.fc2(patches)\n",
    "        patches = patches.transpose(1, 2)  # (B, patch_dim, num_patches)\n",
    "\n",
    "        # Fold back to spatial map\n",
    "        new_H = H // self.patch_h * self.patch_h\n",
    "        new_W = W // self.patch_w * self.patch_w\n",
    "        folded = F.fold(patches, output_size=(new_H, new_W),\n",
    "                        kernel_size=(self.patch_h, self.patch_w),\n",
    "                        stride=(self.patch_h, self.patch_w))\n",
    "\n",
    "        # Fusion + Residual\n",
    "        out = self.fusion(folded)\n",
    "        out = out + x\n",
    "        return out\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads=4, mlp_ratio=4.0, dropout=0.0):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = nn.MultiheadAttention(dim, num_heads, dropout=dropout,\n",
    "                                          batch_first=True)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_norm = self.norm1(x)\n",
    "        attn_out, _ = self.attn(x_norm, x_norm, x_norm)\n",
    "        x = x + attn_out\n",
    "        x_norm = self.norm2(x)\n",
    "        mlp_out = self.mlp(x_norm)\n",
    "        x = x + mlp_out\n",
    "        return x\n",
    "\n",
    "# Encoder Block with Depthwise Separable Conv and ULSAM\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, use_ulsam=True, num_subspaces=4):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        \n",
    "        self.use_ulsam = use_ulsam\n",
    "        \n",
    "        # Downsampling with depthwise separable conv\n",
    "        self.downsample = nn.Sequential(\n",
    "            DepthwiseSeparableConv(in_channels, out_channels, kernel_size=3, \n",
    "                                  stride=2, padding=1),\n",
    "            DepthwiseSeparableConv(out_channels, out_channels, kernel_size=3, \n",
    "                                  stride=1, padding=1)\n",
    "        )\n",
    "        \n",
    "        # ULSAM for attention\n",
    "        if self.use_ulsam:\n",
    "            self.ulsam = ULSAM(out_channels, num_subspaces)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.downsample(x)\n",
    "        \n",
    "        if self.use_ulsam:\n",
    "            out = self.ulsam(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "# Decoder Block\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, skip_channels, out_channels):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        # fusion에서 in_channels=업샘플 채널+skip 채널로 잡기\n",
    "        fusion_in = in_channels + skip_channels\n",
    "        self.fusion = nn.Sequential(\n",
    "            DepthwiseSeparableConv(fusion_in, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            DepthwiseSeparableConv(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x_up = self.upsample(x)\n",
    "        if x_up.size()[2:] != skip.size()[2:]:\n",
    "            x_up = F.interpolate(x_up, size=skip.shape[2:], mode='bilinear', align_corners=True)\n",
    "        x_concat = torch.cat([x_up, skip], dim=1)\n",
    "        out = self.fusion(x_concat)\n",
    "        return out\n",
    "\n",
    "# Segmentation Head\n",
    "class SegHead(nn.Module):\n",
    "    def __init__(self, inplanes, interplanes, outplanes, aux_head=False):\n",
    "        super(SegHead, self).__init__()\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        if aux_head:\n",
    "            # Auxiliary head (no upsampling)\n",
    "            self.conv_bn_relu = nn.Sequential(\n",
    "                nn.Conv2d(inplanes, interplanes, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(interplanes),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        else:\n",
    "            # Main head (with upsampling)\n",
    "            self.conv_bn_relu = nn.Sequential(\n",
    "                nn.ConvTranspose2d(inplanes, interplanes, kernel_size=3, stride=2, \n",
    "                                  padding=1, output_padding=1),\n",
    "                nn.BatchNorm2d(interplanes),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        \n",
    "        self.conv = nn.Conv2d(interplanes, outplanes, kernel_size=1, stride=1, padding=0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv_bn_relu(x)\n",
    "        out = self.conv(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T11:27:22.581758Z",
     "iopub.status.busy": "2025-10-13T11:27:22.581581Z",
     "iopub.status.idle": "2025-10-13T11:27:22.596935Z",
     "shell.execute_reply": "2025-10-13T11:27:22.596301Z",
     "shell.execute_reply.started": "2025-10-13T11:27:22.581742Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def binary_metrics(preds, targets, eps=1e-6):\n",
    "    preds = preds.float()\n",
    "    targets = targets.float()\n",
    "\n",
    "    tp = (preds * targets).sum(dim=(1,2,3))\n",
    "    fp = (preds * (1 - targets)).sum(dim=(1,2,3))\n",
    "    fn = ((1 - preds) * targets).sum(dim=(1,2,3))\n",
    "\n",
    "    precision = (tp + eps) / (tp + fp + eps)\n",
    "    recall    = (tp + eps) / (tp + fn + eps)\n",
    "    f1        = (2 * precision * recall + eps) / (precision + recall + eps)  # Dice\n",
    "    union     = tp + fp + fn\n",
    "    iou       = (tp + eps) / (union + eps)\n",
    "\n",
    "    return {\n",
    "        \"iou\": iou.mean().item(),\n",
    "        \"precision\": precision.mean().item(),\n",
    "        \"recall\": recall.mean().item(),\n",
    "        \"f1\": f1.mean().item(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# --- [추가] F-beta 손실 함수 정의 ---\n",
    "class FbetaLoss(nn.Module):\n",
    "    def __init__(self, beta=2.0, smooth=1e-6):\n",
    "        super(FbetaLoss, self).__init__()\n",
    "        self.beta = beta\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        preds = torch.sigmoid(logits)\n",
    "\n",
    "        tp = (preds * targets).sum(dim=(2, 3))\n",
    "        fp = (preds * (1 - targets)).sum(dim=(2, 3))\n",
    "        fn = ((1 - preds) * targets).sum(dim=(2, 3))\n",
    "\n",
    "        beta2 = self.beta ** 2\n",
    "        f_beta = ((1 + beta2) * tp + self.smooth) / ((1 + beta2) * tp + beta2 * fn + fp + self.smooth)\n",
    "        \n",
    "        return 1 - f_beta.mean()\n",
    "\n",
    "# --- [추가] F-beta 점수 계산 함수 (검증용) ---\n",
    "def calculate_fbeta(preds, masks, beta=2.0, smooth=1e-6):\n",
    "    preds = preds.float()\n",
    "    masks = masks.float()\n",
    "\n",
    "    # 배치 전체를 하나의 큰 이미지로 보고 계산\n",
    "    tp = (preds * masks).sum()\n",
    "    fp = (preds * (1 - masks)).sum()\n",
    "    fn = ((1 - preds) * masks).sum()\n",
    "    \n",
    "    beta2 = beta ** 2\n",
    "    f_beta = ((1 + beta2) * tp + smooth) / ((1 + beta2) * tp + beta2 * fn + fp + smooth)\n",
    "    \n",
    "    return f_beta.item()\n",
    "\n",
    "# --- [적용] 사용자께서 제공하신 binary_metrics 함수 ---\n",
    "def binary_metrics(preds, targets, eps=1e-6):\n",
    "    preds = preds.float()\n",
    "    targets = targets.float()\n",
    "\n",
    "    # 배치 내 각 샘플에 대해 TP, FP, FN 계산\n",
    "    tp = (preds * targets).sum(dim=(1,2,3))\n",
    "    fp = (preds * (1 - targets)).sum(dim=(1,2,3))\n",
    "    fn = ((1 - preds) * targets).sum(dim=(1,2,3))\n",
    "\n",
    "    precision = (tp + eps) / (tp + fp + eps)\n",
    "    recall    = (tp + eps) / (tp + fn + eps)\n",
    "    f1        = (2 * precision * recall + eps) / (precision + recall + eps)\n",
    "    union     = tp + fp + fn\n",
    "    iou       = (tp + eps) / (union + eps)\n",
    "\n",
    "    # 각 지표를 배치에 대해 평균내어 반환\n",
    "    return {\n",
    "        \"iou\": iou.mean().item(),\n",
    "        \"precision\": precision.mean().item(),\n",
    "        \"recall\": recall.mean().item(),\n",
    "        \"f1\": f1.mean().item(),\n",
    "    }\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# train_model 함수\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    device,\n",
    "    epochs=10,\n",
    "    aux_weights=(1.0, 0.4, 0.4),\n",
    "    lr=1e-3,\n",
    "    use_amp=False,\n",
    "    log_every=500,\n",
    "    validate_every_steps=None,\n",
    "    threshold=0.5,\n",
    "    patience=5,\n",
    "    model_save_path='best_model.pth'\n",
    "):\n",
    "    model.to(device)\n",
    "    criterion = FbetaLoss(beta=2.0)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "    # --- [추가] Early Stopping을 위한 변수 초기화 ---\n",
    "    patience_counter = 0\n",
    "    best_val_fbeta = 0.0\n",
    "\n",
    "    global_step = 0\n",
    "    # --- [복원] Step 단위 로깅을 위한 변수 ---\n",
    "    win_loss, win_iou, win_f1, win_steps = 0.0, 0.0, 0.0, 0\n",
    "    t0 = time.time()\n",
    "\n",
    "    best_threshold = threshold  # F2 score 최적화 위해 threshold 저장 변수 추가\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for imgs, masks in train_loader:\n",
    "            global_step += 1\n",
    "            imgs = imgs.to(device, non_blocking=True)\n",
    "            masks = masks.to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "                outputs = model(imgs)\n",
    "                main_logit = outputs[0] if isinstance(outputs, (list, tuple)) else outputs\n",
    "                \n",
    "                loss = aux_weights[0] * criterion(main_logit, masks)\n",
    "                if isinstance(outputs, (list, tuple)):\n",
    "                    if len(outputs) > 1 and aux_weights[1] > 0:\n",
    "                        loss = loss + aux_weights[1] * criterion(outputs[1], masks)\n",
    "                    if len(outputs) > 2 and aux_weights[2] > 0:\n",
    "                        loss = loss + aux_weights[2] * criterion(outputs[2], masks)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # --- [복원] Step 단위 로깅을 위한 집계 로직 ---\n",
    "            win_loss += loss.item()\n",
    "            win_steps += 1\n",
    "            with torch.no_grad():\n",
    "                probs = torch.sigmoid(main_logit)\n",
    "                preds = (probs > best_threshold).float()\n",
    "                m = binary_metrics(preds, masks)\n",
    "                win_iou += m[\"iou\"]\n",
    "                win_f1  += m[\"f1\"]\n",
    "            \n",
    "            # --- [복원] Step 단위 로깅 출력문 ---\n",
    "            if log_every and (global_step % log_every == 0):\n",
    "                elapsed = time.time() - t0\n",
    "                lr_now = optimizer.param_groups[0][\"lr\"]\n",
    "                print(f\"[Step {global_step}] epoch={epoch}  \"\n",
    "                      f\"avg_loss(win)={win_loss/max(1,win_steps):.4f}  \"\n",
    "                      f\"avg_iou(win)={win_iou/max(1,win_steps):.4f}  \"\n",
    "                      f\"avg_f1(win)={win_f1/max(1,win_steps):.4f}  \"\n",
    "                      f\"lr={lr_now:.3e}  elapsed={elapsed:.1f}s\")\n",
    "                win_loss = win_iou = win_f1 = 0.0\n",
    "                win_steps = 0\n",
    "                t0 = time.time()\n",
    "\n",
    "        # --- Epoch 종료 후 검증 ---\n",
    "        avg_train_loss = epoch_loss / max(1, len(train_loader))\n",
    "        model.eval()\n",
    "\n",
    "        val_fbeta_list, val_loss_list, val_iou_list, val_f1_list = [], [], [], []\n",
    "\n",
    "        all_probs = []\n",
    "        all_masks = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for imgs, masks in val_loader:\n",
    "                imgs = imgs.to(device, non_blocking=True)\n",
    "                masks = masks.to(device, non_blocking=True)\n",
    "\n",
    "                logits_list = model(imgs)\n",
    "                main_logit = logits_list[0] if isinstance(logits_list, (list, tuple)) else logits_list\n",
    "\n",
    "                loss = criterion(main_logit, masks)\n",
    "                val_loss_list.append(loss.item())\n",
    "\n",
    "                probs = torch.sigmoid(main_logit)\n",
    "                all_probs.append(probs.cpu())\n",
    "                all_masks.append(masks.cpu())\n",
    "\n",
    "                preds = (probs > best_threshold).float()\n",
    "\n",
    "                fbeta_score = calculate_fbeta(preds, masks, beta=2.0)\n",
    "                val_fbeta_list.append(fbeta_score)\n",
    "\n",
    "                m = binary_metrics(preds, masks)\n",
    "                val_iou_list.append(m[\"iou\"])\n",
    "                val_f1_list.append(m[\"f1\"])\n",
    "\n",
    "        all_probs = torch.cat(all_probs, dim=0).numpy()\n",
    "        all_masks = torch.cat(all_masks, dim=0).numpy()\n",
    "\n",
    "        # --- Threshold 최적화 (F2 score 기준) ---\n",
    "        thresholds = np.linspace(0.01, 0.99, 99)\n",
    "        best_score = 0.0\n",
    "        best_epoch_threshold = best_threshold\n",
    "\n",
    "        for t in thresholds:\n",
    "            preds_bin = (all_probs > t).astype(np.uint8)\n",
    "            tp = (preds_bin * all_masks).sum(axis=(1,2,3))\n",
    "            fp = (preds_bin * (1 - all_masks)).sum(axis=(1,2,3))\n",
    "            fn = ((1 - preds_bin) * all_masks).sum(axis=(1,2,3))\n",
    "\n",
    "            beta2 = 2.0 ** 2\n",
    "            f_beta = ((1 + beta2) * tp + 1e-6) / ((1 + beta2) * tp + beta2 * fn + fp + 1e-6)\n",
    "            score = f_beta.mean()\n",
    "\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_epoch_threshold = t\n",
    "        # --- Threshold 최적화 끝 ---\n",
    "\n",
    "        avg_val_loss = np.mean(val_loss_list)\n",
    "        avg_val_fbeta = np.mean(val_fbeta_list)\n",
    "        avg_val_iou = np.mean(val_iou_list)\n",
    "        avg_val_f1 = np.mean(val_f1_list)\n",
    "\n",
    "        # --- [복원] Epoch 단위 출력문에 F2 score, 최적 threshold 출력 추가 ---\n",
    "        print(f\"[Epoch {epoch}/{epochs}] \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "              f\"Val Loss: {avg_val_loss:.4f} | \"\n",
    "              f\"Val IoU: {avg_val_iou:.4f} | \"\n",
    "              f\"Val F1: {avg_val_f1:.4f} | \"\n",
    "              f\"Val F-beta(β=2.0): {avg_val_fbeta:.4f} | \"\n",
    "              f\"Best Threshold: {best_epoch_threshold:.3f} | \"\n",
    "              f\"Best F-beta@threshold: {best_score:.4f}\")\n",
    "\n",
    "        # Early Stopping 로직 (F-beta 기준)\n",
    "        if best_score > best_val_fbeta:\n",
    "            best_val_fbeta = best_score\n",
    "            best_threshold = best_epoch_threshold\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f\" -> Best score updated. Model saved.\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\" -> Patience: {patience_counter}/{patience}\")\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping triggered after {patience} epochs without improvement.\")\n",
    "            break\n",
    "            \n",
    "    print(f\"\\nTraining finished. Best Val F-beta(β=2.0) was: {best_val_fbeta:.4f}\")\n",
    "    print(f\"Best Threshold found: {best_threshold:.3f}\")\n",
    "\n",
    "    model.load_state_dict(torch.load(model_save_path))\n",
    "    return model, best_threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T11:27:22.597755Z",
     "iopub.status.busy": "2025-10-13T11:27:22.597539Z",
     "iopub.status.idle": "2025-10-13T11:27:22.853695Z",
     "shell.execute_reply": "2025-10-13T11:27:22.853224Z",
     "shell.execute_reply.started": "2025-10-13T11:27:22.597730Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load..tarin data\n",
      "Loaded train data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jms/.local/lib/python3.12/site-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n",
      "/tmp/ipykernel_2128/286807507.py:15: UserWarning: Argument(s) 'variance' are not valid for transform GaussNoise\n",
      "  GaussNoise(variance=(10.0, 40.0)),\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CrackDataset(TRAIN_DIR, transform=get_transform(), augment_ratio=2)\n",
    "val_dataset = CrackDataset(VAL_DIR, transform=None, augment_ratio=1)\n",
    "\n",
    "print(\"Load..tarin data\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "print(\"Loaded train data\")\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T11:27:22.854526Z",
     "iopub.status.busy": "2025-10-13T11:27:22.854362Z",
     "iopub.status.idle": "2025-10-13T11:27:27.325732Z",
     "shell.execute_reply": "2025-10-13T11:27:27.324895Z",
     "shell.execute_reply.started": "2025-10-13T11:27:22.854513Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 290.38 k\n",
      "Total MACs: 392.74 MMac\n"
     ]
    }
   ],
   "source": [
    "from ptflops import get_model_complexity_info\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EfficientCrackNet().to(device)\n",
    "input_size = (1, 192, 192)\n",
    "\n",
    "macs, params = get_model_complexity_info(model,\n",
    "                                         input_size,\n",
    "                                         as_strings=True,\n",
    "                                         print_per_layer_stat=False,\n",
    "                                         verbose=False)\n",
    "print(f\"Total Params: {params}\")\n",
    "print(f\"Total MACs: {macs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T11:27:27.326740Z",
     "iopub.status.busy": "2025-10-13T11:27:27.326532Z",
     "iopub.status.idle": "2025-10-13T11:40:59.805475Z",
     "shell.execute_reply": "2025-10-13T11:40:59.804776Z",
     "shell.execute_reply.started": "2025-10-13T11:27:27.326723Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2128/1287974975.py:87: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
      "/tmp/ipykernel_2128/1287974975.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "[Step 500] epoch=1  avg_loss(win)=1.3739  avg_iou(win)=0.1064  avg_f1(win)=0.1865  lr=1.000e-03  elapsed=15.1s\n",
      "[Step 1000] epoch=1  avg_loss(win)=1.1518  avg_iou(win)=0.2018  avg_f1(win)=0.3159  lr=1.000e-03  elapsed=14.1s\n",
      "[Step 1500] epoch=1  avg_loss(win)=1.0756  avg_iou(win)=0.2376  avg_f1(win)=0.3648  lr=1.000e-03  elapsed=13.8s\n",
      "[Epoch 1/40] Train Loss: 1.1737 | Val Loss: 0.4994 | Val IoU: 0.3225 | Val F1: 0.4537 | Val F-beta(β=2.0): 0.6360 | Best Threshold: 0.980 | Best F-beta@threshold: 0.5250\n",
      " -> Best score updated. Model saved.\n",
      "[Step 2000] epoch=2  avg_loss(win)=1.0600  avg_iou(win)=0.2553  avg_f1(win)=0.3847  lr=1.000e-03  elapsed=48.7s\n",
      "[Step 2500] epoch=2  avg_loss(win)=1.0174  avg_iou(win)=0.2866  avg_f1(win)=0.4249  lr=1.000e-03  elapsed=13.5s\n",
      "[Step 3000] epoch=2  avg_loss(win)=1.0029  avg_iou(win)=0.2848  avg_f1(win)=0.4165  lr=1.000e-03  elapsed=13.1s\n",
      "[Step 3500] epoch=2  avg_loss(win)=0.9918  avg_iou(win)=0.2920  avg_f1(win)=0.4260  lr=1.000e-03  elapsed=13.0s\n",
      "[Epoch 2/40] Train Loss: 1.0023 | Val Loss: 0.4826 | Val IoU: 0.3601 | Val F1: 0.4892 | Val F-beta(β=2.0): 0.6468 | Best Threshold: 0.990 | Best F-beta@threshold: 0.5398\n",
      " -> Best score updated. Model saved.\n",
      "[Step 4000] epoch=3  avg_loss(win)=0.9886  avg_iou(win)=0.2966  avg_f1(win)=0.4323  lr=1.000e-03  elapsed=47.5s\n",
      "[Step 4500] epoch=3  avg_loss(win)=0.9873  avg_iou(win)=0.2952  avg_f1(win)=0.4248  lr=1.000e-03  elapsed=12.9s\n",
      "[Step 5000] epoch=3  avg_loss(win)=0.9517  avg_iou(win)=0.3154  avg_f1(win)=0.4499  lr=1.000e-03  elapsed=12.8s\n",
      "[Step 5500] epoch=3  avg_loss(win)=0.9647  avg_iou(win)=0.3106  avg_f1(win)=0.4445  lr=1.000e-03  elapsed=12.5s\n",
      "[Epoch 3/40] Train Loss: 0.9702 | Val Loss: 0.4733 | Val IoU: 0.3533 | Val F1: 0.4855 | Val F-beta(β=2.0): 0.6604 | Best Threshold: 0.955 | Best F-beta@threshold: 0.5440\n",
      " -> Best score updated. Model saved.\n",
      "[Step 6000] epoch=4  avg_loss(win)=0.9569  avg_iou(win)=0.3066  avg_f1(win)=0.4376  lr=1.000e-03  elapsed=47.2s\n",
      "[Step 6500] epoch=4  avg_loss(win)=0.9559  avg_iou(win)=0.3076  avg_f1(win)=0.4390  lr=1.000e-03  elapsed=12.7s\n",
      "[Step 7000] epoch=4  avg_loss(win)=0.9444  avg_iou(win)=0.3071  avg_f1(win)=0.4367  lr=1.000e-03  elapsed=12.9s\n",
      "[Step 7500] epoch=4  avg_loss(win)=0.9346  avg_iou(win)=0.3143  avg_f1(win)=0.4426  lr=1.000e-03  elapsed=12.5s\n",
      "[Epoch 4/40] Train Loss: 0.9471 | Val Loss: 0.4766 | Val IoU: 0.3971 | Val F1: 0.5223 | Val F-beta(β=2.0): 0.6461 | Best Threshold: 0.935 | Best F-beta@threshold: 0.5692\n",
      " -> Best score updated. Model saved.\n",
      "[Step 8000] epoch=5  avg_loss(win)=0.9397  avg_iou(win)=0.3195  avg_f1(win)=0.4498  lr=1.000e-03  elapsed=47.8s\n",
      "[Step 8500] epoch=5  avg_loss(win)=0.9453  avg_iou(win)=0.3063  avg_f1(win)=0.4354  lr=1.000e-03  elapsed=13.0s\n",
      "[Step 9000] epoch=5  avg_loss(win)=0.9227  avg_iou(win)=0.3184  avg_f1(win)=0.4527  lr=1.000e-03  elapsed=12.5s\n",
      "[Epoch 5/40] Train Loss: 0.9344 | Val Loss: 0.4636 | Val IoU: 0.3633 | Val F1: 0.4904 | Val F-beta(β=2.0): 0.6698 | Best Threshold: 0.975 | Best F-beta@threshold: 0.5553\n",
      " -> Patience: 1/5\n",
      "[Step 9500] epoch=6  avg_loss(win)=0.9363  avg_iou(win)=0.3133  avg_f1(win)=0.4420  lr=1.000e-03  elapsed=47.9s\n",
      "[Step 10000] epoch=6  avg_loss(win)=0.9403  avg_iou(win)=0.3104  avg_f1(win)=0.4401  lr=1.000e-03  elapsed=12.8s\n",
      "[Step 10500] epoch=6  avg_loss(win)=0.9227  avg_iou(win)=0.3220  avg_f1(win)=0.4513  lr=1.000e-03  elapsed=13.1s\n",
      "[Step 11000] epoch=6  avg_loss(win)=0.9072  avg_iou(win)=0.3240  avg_f1(win)=0.4576  lr=1.000e-03  elapsed=12.7s\n",
      "[Epoch 6/40] Train Loss: 0.9244 | Val Loss: 0.4477 | Val IoU: 0.3614 | Val F1: 0.4907 | Val F-beta(β=2.0): 0.6887 | Best Threshold: 0.990 | Best F-beta@threshold: 0.5635\n",
      " -> Patience: 2/5\n",
      "[Step 11500] epoch=7  avg_loss(win)=0.9203  avg_iou(win)=0.3237  avg_f1(win)=0.4522  lr=1.000e-03  elapsed=47.9s\n",
      "[Step 12000] epoch=7  avg_loss(win)=0.9052  avg_iou(win)=0.3288  avg_f1(win)=0.4596  lr=1.000e-03  elapsed=12.9s\n",
      "[Step 12500] epoch=7  avg_loss(win)=0.9201  avg_iou(win)=0.3148  avg_f1(win)=0.4444  lr=1.000e-03  elapsed=14.0s\n",
      "[Step 13000] epoch=7  avg_loss(win)=0.9204  avg_iou(win)=0.3167  avg_f1(win)=0.4502  lr=1.000e-03  elapsed=12.7s\n",
      "[Epoch 7/40] Train Loss: 0.9140 | Val Loss: 0.4466 | Val IoU: 0.3642 | Val F1: 0.4920 | Val F-beta(β=2.0): 0.6860 | Best Threshold: 0.985 | Best F-beta@threshold: 0.5678\n",
      " -> Patience: 3/5\n",
      "[Step 13500] epoch=8  avg_loss(win)=0.9012  avg_iou(win)=0.3273  avg_f1(win)=0.4565  lr=1.000e-03  elapsed=50.0s\n",
      "[Step 14000] epoch=8  avg_loss(win)=0.9285  avg_iou(win)=0.3191  avg_f1(win)=0.4484  lr=1.000e-03  elapsed=14.1s\n",
      "[Step 14500] epoch=8  avg_loss(win)=0.8968  avg_iou(win)=0.3320  avg_f1(win)=0.4633  lr=1.000e-03  elapsed=14.3s\n",
      "[Step 15000] epoch=8  avg_loss(win)=0.8926  avg_iou(win)=0.3354  avg_f1(win)=0.4645  lr=1.000e-03  elapsed=14.5s\n",
      "[Epoch 8/40] Train Loss: 0.9056 | Val Loss: 0.4519 | Val IoU: 0.3741 | Val F1: 0.5052 | Val F-beta(β=2.0): 0.6793 | Best Threshold: 0.975 | Best F-beta@threshold: 0.5721\n",
      " -> Best score updated. Model saved.\n",
      "[Step 15500] epoch=9  avg_loss(win)=0.9091  avg_iou(win)=0.3281  avg_f1(win)=0.4574  lr=1.000e-03  elapsed=50.9s\n",
      "[Step 16000] epoch=9  avg_loss(win)=0.8979  avg_iou(win)=0.3318  avg_f1(win)=0.4639  lr=1.000e-03  elapsed=17.5s\n",
      "[Step 16500] epoch=9  avg_loss(win)=0.8969  avg_iou(win)=0.3204  avg_f1(win)=0.4489  lr=1.000e-03  elapsed=15.0s\n",
      "[Epoch 9/40] Train Loss: 0.9016 | Val Loss: 0.4523 | Val IoU: 0.3697 | Val F1: 0.4991 | Val F-beta(β=2.0): 0.6823 | Best Threshold: 0.965 | Best F-beta@threshold: 0.5646\n",
      " -> Patience: 1/5\n",
      "[Step 17000] epoch=10  avg_loss(win)=0.9048  avg_iou(win)=0.3205  avg_f1(win)=0.4510  lr=1.000e-03  elapsed=54.4s\n",
      "[Step 17500] epoch=10  avg_loss(win)=0.9204  avg_iou(win)=0.3111  avg_f1(win)=0.4392  lr=1.000e-03  elapsed=14.4s\n",
      "[Step 18000] epoch=10  avg_loss(win)=0.8865  avg_iou(win)=0.3280  avg_f1(win)=0.4582  lr=1.000e-03  elapsed=16.1s\n",
      "[Step 18500] epoch=10  avg_loss(win)=0.8961  avg_iou(win)=0.3573  avg_f1(win)=0.4927  lr=1.000e-03  elapsed=14.5s\n",
      "[Epoch 10/40] Train Loss: 0.9020 | Val Loss: 0.3726 | Val IoU: 0.4579 | Val F1: 0.5727 | Val F-beta(β=2.0): 0.6420 | Best Threshold: 0.821 | Best F-beta@threshold: 0.6303\n",
      " -> Best score updated. Model saved.\n",
      "[Step 19000] epoch=11  avg_loss(win)=0.8855  avg_iou(win)=0.3706  avg_f1(win)=0.5010  lr=1.000e-03  elapsed=52.0s\n",
      "[Step 19500] epoch=11  avg_loss(win)=0.9006  avg_iou(win)=0.3553  avg_f1(win)=0.4827  lr=1.000e-03  elapsed=14.0s\n",
      "[Step 20000] epoch=11  avg_loss(win)=0.9124  avg_iou(win)=0.3586  avg_f1(win)=0.4799  lr=1.000e-03  elapsed=13.9s\n",
      "[Step 20500] epoch=11  avg_loss(win)=0.8695  avg_iou(win)=0.3641  avg_f1(win)=0.4900  lr=1.000e-03  elapsed=13.7s\n",
      "[Epoch 11/40] Train Loss: 0.8872 | Val Loss: 0.3840 | Val IoU: 0.4447 | Val F1: 0.5655 | Val F-beta(β=2.0): 0.6331 | Best Threshold: 0.020 | Best F-beta@threshold: 0.6187\n",
      " -> Patience: 1/5\n",
      "[Step 21000] epoch=12  avg_loss(win)=0.8807  avg_iou(win)=0.3780  avg_f1(win)=0.4999  lr=1.000e-03  elapsed=49.6s\n",
      "[Step 21500] epoch=12  avg_loss(win)=0.8425  avg_iou(win)=0.3902  avg_f1(win)=0.5160  lr=1.000e-03  elapsed=13.7s\n",
      "[Step 22000] epoch=12  avg_loss(win)=0.8504  avg_iou(win)=0.3858  avg_f1(win)=0.5144  lr=1.000e-03  elapsed=13.3s\n",
      "[Step 22500] epoch=12  avg_loss(win)=0.8412  avg_iou(win)=0.3987  avg_f1(win)=0.5172  lr=1.000e-03  elapsed=14.6s\n",
      "[Epoch 12/40] Train Loss: 0.8469 | Val Loss: 0.3483 | Val IoU: 0.4672 | Val F1: 0.5840 | Val F-beta(β=2.0): 0.6813 | Best Threshold: 0.781 | Best F-beta@threshold: 0.6545\n",
      " -> Best score updated. Model saved.\n",
      "[Step 23000] epoch=13  avg_loss(win)=0.8717  avg_iou(win)=0.3745  avg_f1(win)=0.5020  lr=1.000e-03  elapsed=54.9s\n",
      "[Step 23500] epoch=13  avg_loss(win)=0.9127  avg_iou(win)=0.3621  avg_f1(win)=0.4844  lr=1.000e-03  elapsed=15.5s\n",
      "[Step 24000] epoch=13  avg_loss(win)=0.9640  avg_iou(win)=0.3310  avg_f1(win)=0.4521  lr=1.000e-03  elapsed=15.5s\n",
      "[Step 24500] epoch=13  avg_loss(win)=0.8833  avg_iou(win)=0.3710  avg_f1(win)=0.4965  lr=1.000e-03  elapsed=15.7s\n",
      "[Epoch 13/40] Train Loss: 0.9133 | Val Loss: 0.3700 | Val IoU: 0.4576 | Val F1: 0.5771 | Val F-beta(β=2.0): 0.6374 | Best Threshold: 0.005 | Best F-beta@threshold: 0.6334\n",
      " -> Patience: 1/5\n",
      "[Step 25000] epoch=14  avg_loss(win)=0.8852  avg_iou(win)=0.3680  avg_f1(win)=0.4912  lr=1.000e-03  elapsed=55.0s\n",
      "[Step 25500] epoch=14  avg_loss(win)=0.8358  avg_iou(win)=0.3884  avg_f1(win)=0.5096  lr=1.000e-03  elapsed=13.0s\n",
      "[Step 26000] epoch=14  avg_loss(win)=0.8541  avg_iou(win)=0.3838  avg_f1(win)=0.5042  lr=1.000e-03  elapsed=12.7s\n",
      "[Epoch 14/40] Train Loss: 0.8616 | Val Loss: 0.3683 | Val IoU: 0.4559 | Val F1: 0.5773 | Val F-beta(β=2.0): 0.6406 | Best Threshold: 0.070 | Best F-beta@threshold: 0.6326\n",
      " -> Patience: 2/5\n",
      "[Step 26500] epoch=15  avg_loss(win)=0.8636  avg_iou(win)=0.3851  avg_f1(win)=0.5070  lr=1.000e-03  elapsed=47.2s\n",
      "[Step 27000] epoch=15  avg_loss(win)=0.8756  avg_iou(win)=0.3737  avg_f1(win)=0.4951  lr=1.000e-03  elapsed=12.9s\n",
      "[Step 27500] epoch=15  avg_loss(win)=0.8832  avg_iou(win)=0.3740  avg_f1(win)=0.4985  lr=1.000e-03  elapsed=12.7s\n",
      "[Step 28000] epoch=15  avg_loss(win)=0.8787  avg_iou(win)=0.3652  avg_f1(win)=0.4939  lr=1.000e-03  elapsed=12.7s\n",
      "[Epoch 15/40] Train Loss: 0.8676 | Val Loss: 0.3658 | Val IoU: 0.4380 | Val F1: 0.5654 | Val F-beta(β=2.0): 0.6592 | Best Threshold: 0.935 | Best F-beta@threshold: 0.6356\n",
      " -> Patience: 3/5\n",
      "[Step 28500] epoch=16  avg_loss(win)=0.8245  avg_iou(win)=0.3893  avg_f1(win)=0.5180  lr=1.000e-03  elapsed=47.8s\n",
      "[Step 29000] epoch=16  avg_loss(win)=0.8272  avg_iou(win)=0.3929  avg_f1(win)=0.5186  lr=1.000e-03  elapsed=12.7s\n",
      "[Step 29500] epoch=16  avg_loss(win)=0.8273  avg_iou(win)=0.3863  avg_f1(win)=0.5191  lr=1.000e-03  elapsed=12.4s\n",
      "[Step 30000] epoch=16  avg_loss(win)=0.8306  avg_iou(win)=0.3867  avg_f1(win)=0.5182  lr=1.000e-03  elapsed=12.3s\n",
      "[Epoch 16/40] Train Loss: 0.8314 | Val Loss: 0.3620 | Val IoU: 0.4554 | Val F1: 0.5735 | Val F-beta(β=2.0): 0.6595 | Best Threshold: 0.189 | Best F-beta@threshold: 0.6394\n",
      " -> Patience: 4/5\n",
      "[Step 30500] epoch=17  avg_loss(win)=0.8733  avg_iou(win)=0.3736  avg_f1(win)=0.4985  lr=1.000e-03  elapsed=46.9s\n",
      "[Step 31000] epoch=17  avg_loss(win)=0.8481  avg_iou(win)=0.3815  avg_f1(win)=0.5166  lr=1.000e-03  elapsed=12.3s\n",
      "[Step 31500] epoch=17  avg_loss(win)=0.8638  avg_iou(win)=0.3814  avg_f1(win)=0.5063  lr=1.000e-03  elapsed=12.4s\n",
      "[Step 32000] epoch=17  avg_loss(win)=0.8832  avg_iou(win)=0.3685  avg_f1(win)=0.4934  lr=1.000e-03  elapsed=12.2s\n",
      "[Epoch 17/40] Train Loss: 0.8713 | Val Loss: 0.4130 | Val IoU: 0.4235 | Val F1: 0.5291 | Val F-beta(β=2.0): 0.6003 | Best Threshold: 0.667 | Best F-beta@threshold: 0.5873\n",
      " -> Patience: 5/5\n",
      "\n",
      "Early stopping triggered after 10 epochs without improvement.\n",
      "\n",
      "Training finished. Best Val F-beta(β=2.0) was: 0.6545\n",
      "Best Threshold found: 0.781\n"
     ]
    }
   ],
   "source": [
    "print(device)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#model = EfficientCrackNet().to(device)\n",
    "model, best_threshold = train_model(model, train_loader, val_loader, device, epochs=100, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_mask_image(\n",
    "    mask_image: Image.Image,\n",
    "    base_output_dir: str,\n",
    "    original_filename: str,\n",
    "    script_name: str = 'defalut'\n",
    "):\n",
    "    \"\"\"\n",
    "    마스크 이미지를 지정된 규칙에 따라 폴더를 생성하고 저장합니다.\n",
    "\n",
    "    Args:\n",
    "        mask_image (Image.Image): 저장할 PIL 이미지 객체.\n",
    "        base_output_dir (str): 결과 폴더를 생성할 상위 경로.\n",
    "        original_filename (str): 원본 이미지 파일명 (e.g., 'image_001.jpg').\n",
    "        script_name (str): 현재 실행 중인 파이썬 스크립트 또는 노트북 파일명.\n",
    "    \n",
    "    Returns:\n",
    "        str: 파일이 저장된 전체 경로.\n",
    "    \"\"\"\n",
    "    # 1. 'test_파일명_mmddhhmm' 형식으로 폴더명 생성\n",
    "    now = datetime.datetime.now()\n",
    "    timestamp = now.strftime(\"%m%d%H%M\")  # mmddhhmm 형식\n",
    "    \n",
    "    # 스크립트 이름에서 확장자(.py, .ipynb) 제거\n",
    "    script_basename = os.path.splitext(script_name)[0]\n",
    "    \n",
    "    folder_name = f\"test_{script_basename}_{timestamp}\"\n",
    "    output_dir = os.path.join(base_output_dir, folder_name)\n",
    "    \n",
    "    # 폴더 생성 (이미 존재하면 그대로 사용)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # 2. 저장할 파일명 생성 (원본 파일명 기반)\n",
    "    original_basename = os.path.splitext(original_filename)[0]\n",
    "    output_filename = f\"{original_basename}_mask.png\"\n",
    "    \n",
    "    # 3. 전체 저장 경로를 조합하고 이미지 저장\n",
    "    output_path = os.path.join(output_dir, output_filename)\n",
    "    mask_image.save(output_path)\n",
    "    \n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T11:40:59.806608Z",
     "iopub.status.busy": "2025-10-13T11:40:59.806413Z",
     "iopub.status.idle": "2025-10-13T11:41:00.075814Z",
     "shell.execute_reply": "2025-10-13T11:41:00.075230Z",
     "shell.execute_reply.started": "2025-10-13T11:40:59.806593Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def rle_encode(mask):\n",
    "    \"\"\"\n",
    "    mask: 2D numpy array of {0,1}, shape (H,W)\n",
    "    return: run length as string\n",
    "    \"\"\"\n",
    "    pixels = mask.flatten(order=\"C\")\n",
    "    ones = np.where(pixels == 1)[0] + 1  # 1-based\n",
    "    if len(ones) == 0:\n",
    "        return \"\"\n",
    "    runs = []\n",
    "    prev = -2\n",
    "    for idx in ones:\n",
    "        if idx > prev + 1:\n",
    "            runs.extend((idx, 0))\n",
    "        runs[-1] += 1\n",
    "        prev = idx\n",
    "    return \" \".join(map(str, runs))\n",
    "\n",
    "\n",
    "def predict_and_submit(model, test_img_dir, output_csv, device, threshold=0.5):\n",
    "    model.eval()\n",
    "    ids, rles = [], []\n",
    "\n",
    "    test_imgs = sorted(glob.glob(os.path.join(test_img_dir, \"*.jpg\")))\n",
    "    for path in test_imgs:\n",
    "        img_id = os.path.splitext(os.path.basename(path))[0]\n",
    "        img = Image.open(path).convert(\"L\")\n",
    "        arr = np.array(img, dtype=np.float32) / 255.0\n",
    "        tensor = torch.tensor(arr).unsqueeze(0).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out_list = model(tensor)\n",
    "            main_logit = out_list[0] if isinstance(out_list, (list, tuple)) else out_list\n",
    "            prob = torch.sigmoid(main_logit)[0,0].cpu().numpy()\n",
    "            pred = (prob > threshold).astype(np.uint8)\n",
    "        \n",
    "        rle = rle_encode(pred)\n",
    "        ids.append(img_id)\n",
    "        rles.append(rle)\n",
    "\n",
    "    df = pd.DataFrame({\"image_id\": ids, \"rle\": rles})\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"[OK] submission saved to {output_csv}, total {len(df)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rle_encode(mask):\n",
    "    \"\"\"\n",
    "    mask: 2D numpy array of {0,1}, shape (H,W)\n",
    "    return: run length as string\n",
    "    \"\"\"\n",
    "    pixels = mask.flatten(order=\"C\")\n",
    "    ones = np.where(pixels == 1)[0] + 1  # 1-based\n",
    "    if len(ones) == 0:\n",
    "        return \"\"\n",
    "    runs = []\n",
    "    prev = -2\n",
    "    for idx in ones:\n",
    "        if idx > prev + 1:\n",
    "            runs.extend((idx, 0))\n",
    "        runs[-1] += 1\n",
    "        prev = idx\n",
    "    return \" \".join(map(str, runs))\n",
    "\n",
    "\n",
    "def predict_submit_and_save_masks(\n",
    "    model, \n",
    "    test_img_dir, \n",
    "    output_csv, \n",
    "    device, \n",
    "    threshold=0.5,\n",
    "    save_masks=False,\n",
    "    mask_save_dir=None\n",
    "):\n",
    "    \n",
    "    model.eval()\n",
    "    ids, rles = [], []\n",
    "\n",
    "    # --- 이미지 저장을 위한 폴더 설정 ---\n",
    "    output_mask_path = \"\"\n",
    "    if save_masks:\n",
    "        if mask_save_dir is None:\n",
    "            # mask_save_dir가 지정되지 않으면 에러 발생\n",
    "            raise ValueError(\"If save_masks is True, mask_save_dir must be provided.\")\n",
    "        \n",
    "        # 현재 시간을 기반으로 하위 폴더 생성\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_mask_path = os.path.join(mask_save_dir, f\"predictions_{timestamp}\")\n",
    "        os.makedirs(output_mask_path, exist_ok=True)\n",
    "        print(f\"Mask images will be saved to: {output_mask_path}\")\n",
    "\n",
    "    test_imgs = sorted(glob.glob(os.path.join(test_img_dir, \"*.jpg\")))\n",
    "    for path in test_imgs:\n",
    "        img_id = os.path.splitext(os.path.basename(path))[0]\n",
    "        img = Image.open(path).convert(\"L\")\n",
    "        arr = np.array(img, dtype=np.float32) / 255.0\n",
    "        tensor = torch.tensor(arr).unsqueeze(0).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out_list = model(tensor)\n",
    "            main_logit = out_list[0] if isinstance(out_list, (list, tuple)) else out_list\n",
    "            prob = torch.sigmoid(main_logit)[0,0].cpu().numpy()\n",
    "            pred = (prob > threshold).astype(np.uint8)\n",
    "        \n",
    "        # ---  마스크 이미지를 파일로 저장 ---\n",
    "        if save_masks:\n",
    "            mask_image = Image.fromarray(pred * 255, mode='L')\n",
    "            mask_filename = f\"{img_id}_mask.png\"\n",
    "            save_path = os.path.join(output_mask_path, mask_filename)\n",
    "            mask_image.save(save_path)\n",
    "\n",
    "        # --- RLE 인코딩 및 CSV 데이터 수집 ---\n",
    "        rle = rle_encode(pred)\n",
    "        ids.append(img_id)\n",
    "        rles.append(rle)\n",
    "\n",
    "    # --- CSV 파일로 최종 저장 ---\n",
    "    df = pd.DataFrame({\"image_id\": ids, \"rle\": rles})\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"OK. Submission CSV saved to {output_csv}, total {len(df)} rows.\")\n",
    "    \n",
    "    if save_masks:\n",
    "        print(f\"OK. Mask images also saved in: {output_mask_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T11:41:00.077800Z",
     "iopub.status.busy": "2025-10-13T11:41:00.077387Z",
     "iopub.status.idle": "2025-10-13T11:41:33.222667Z",
     "shell.execute_reply": "2025-10-13T11:41:33.222039Z",
     "shell.execute_reply.started": "2025-10-13T11:41:00.077781Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask images will be saved to: working/mask_ouputs/predictions_20251015_233222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1323/4196979983.py:61: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
      "  mask_image = Image.fromarray(pred * 255, mode='L')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK. Submission CSV saved to working/submission.csv, total 2667 rows.\n",
      "OK. Mask images also saved in: working/mask_ouputs/predictions_20251015_233222\n"
     ]
    }
   ],
   "source": [
    "predict_submit_and_save_masks(\n",
    "    model=model,\n",
    "    test_img_dir=TEST_DIR,\n",
    "    output_csv=\"working/submission.csv\",\n",
    "    device=device,\n",
    "    save_masks=True,  \n",
    "    mask_save_dir=OUTPUT_MASK,\n",
    "    threshold = best_threshold,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13948799,
     "sourceId": 115856,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
