{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-13T11:26:07.826822Z",
     "iopub.status.busy": "2025-10-13T11:26:07.826549Z",
     "iopub.status.idle": "2025-10-13T11:27:16.075877Z",
     "shell.execute_reply": "2025-10-13T11:27:16.074947Z",
     "shell.execute_reply.started": "2025-10-13T11:26:07.826801Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install -q ptflops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /home/jms/.local/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (2.2.6)\n",
      "Requirement already satisfied: Pillow in /home/jms/.local/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (11.3.0)\n",
      "Requirement already satisfied: scikit-learn in /home/jms/.local/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (1.7.2)\n",
      "Requirement already satisfied: torch in /home/jms/.local/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (2.8.0)\n",
      "Requirement already satisfied: torchvision in /home/jms/.local/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (0.23.0)\n",
      "Requirement already satisfied: ptflops in /home/jms/.local/lib/python3.12/site-packages (from -r requirements.txt (line 6)) (0.7.5)\n",
      "Requirement already satisfied: pandas in /home/jms/.local/lib/python3.12/site-packages (from -r requirements.txt (line 7)) (2.3.3)\n",
      "Requirement already satisfied: segmentation-models-pytorch in /home/jms/.local/lib/python3.12/site-packages (from -r requirements.txt (line 8)) (0.5.0)\n",
      "Requirement already satisfied: albumentations in /home/jms/.local/lib/python3.12/site-packages (from -r requirements.txt (line 9)) (2.0.8)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/jms/.local/lib/python3.12/site-packages (from scikit-learn->-r requirements.txt (line 3)) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/jms/.local/lib/python3.12/site-packages (from scikit-learn->-r requirements.txt (line 3)) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/jms/.local/lib/python3.12/site-packages (from scikit-learn->-r requirements.txt (line 3)) (3.6.0)\n",
      "Requirement already satisfied: filelock in /home/jms/.local/lib/python3.12/site-packages (from torch->-r requirements.txt (line 4)) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/jms/.local/lib/python3.12/site-packages (from torch->-r requirements.txt (line 4)) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from torch->-r requirements.txt (line 4)) (68.1.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/jms/.local/lib/python3.12/site-packages (from torch->-r requirements.txt (line 4)) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/jms/.local/lib/python3.12/site-packages (from torch->-r requirements.txt (line 4)) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch->-r requirements.txt (line 4)) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/jms/.local/lib/python3.12/site-packages (from torch->-r requirements.txt (line 4)) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/jms/.local/lib/python3.12/site-packages (from torch->-r requirements.txt (line 4)) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/jms/.local/lib/python3.12/site-packages (from torch->-r requirements.txt (line 4)) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/jms/.local/lib/python3.12/site-packages (from torch->-r requirements.txt (line 4)) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/jms/.local/lib/python3.12/site-packages (from torch->-r requirements.txt (line 4)) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/jms/.local/lib/python3.12/site-packages (from torch->-r requirements.txt (line 4)) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/jms/.local/lib/python3.12/site-packages (from torch->-r requirements.txt (line 4)) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/jms/.local/lib/python3.12/site-packages (from torch->-r requirements.txt (line 4)) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/jms/.local/lib/python3.12/site-packages (from torch->-r requirements.txt (line 4)) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/jms/.local/lib/python3.12/site-packages (from torch->-r requirements.txt (line 4)) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/jms/.local/lib/python3.12/site-packages (from torch->-r requirements.txt (line 4)) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/jms/.local/lib/python3.12/site-packages (from torch->-r requirements.txt (line 4)) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/jms/.local/lib/python3.12/site-packages (from torch->-r requirements.txt (line 4)) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/jms/.local/lib/python3.12/site-packages (from torch->-r requirements.txt (line 4)) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/jms/.local/lib/python3.12/site-packages (from torch->-r requirements.txt (line 4)) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /home/jms/.local/lib/python3.12/site-packages (from torch->-r requirements.txt (line 4)) (3.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/jms/.local/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 7)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->-r requirements.txt (line 7)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/jms/.local/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 7)) (2025.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.24 in /home/jms/.local/lib/python3.12/site-packages (from segmentation-models-pytorch->-r requirements.txt (line 8)) (0.35.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/jms/.local/lib/python3.12/site-packages (from segmentation-models-pytorch->-r requirements.txt (line 8)) (0.6.2)\n",
      "Requirement already satisfied: timm>=0.9 in /home/jms/.local/lib/python3.12/site-packages (from segmentation-models-pytorch->-r requirements.txt (line 8)) (1.0.20)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/jms/.local/lib/python3.12/site-packages (from segmentation-models-pytorch->-r requirements.txt (line 8)) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in /usr/lib/python3/dist-packages (from albumentations->-r requirements.txt (line 9)) (6.0.1)\n",
      "Requirement already satisfied: pydantic>=2.9.2 in /home/jms/.local/lib/python3.12/site-packages (from albumentations->-r requirements.txt (line 9)) (2.12.2)\n",
      "Requirement already satisfied: albucore==0.0.24 in /home/jms/.local/lib/python3.12/site-packages (from albumentations->-r requirements.txt (line 9)) (0.0.24)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /home/jms/.local/lib/python3.12/site-packages (from albumentations->-r requirements.txt (line 9)) (4.12.0.88)\n",
      "Requirement already satisfied: stringzilla>=3.10.4 in /home/jms/.local/lib/python3.12/site-packages (from albucore==0.0.24->albumentations->-r requirements.txt (line 9)) (4.2.1)\n",
      "Requirement already satisfied: simsimd>=5.9.2 in /home/jms/.local/lib/python3.12/site-packages (from albucore==0.0.24->albumentations->-r requirements.txt (line 9)) (6.5.3)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/jms/.local/lib/python3.12/site-packages (from huggingface-hub>=0.24->segmentation-models-pytorch->-r requirements.txt (line 8)) (25.0)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch->-r requirements.txt (line 8)) (2.31.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/jms/.local/lib/python3.12/site-packages (from huggingface-hub>=0.24->segmentation-models-pytorch->-r requirements.txt (line 8)) (1.1.10)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/jms/.local/lib/python3.12/site-packages (from pydantic>=2.9.2->albumentations->-r requirements.txt (line 9)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /home/jms/.local/lib/python3.12/site-packages (from pydantic>=2.9.2->albumentations->-r requirements.txt (line 9)) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /home/jms/.local/lib/python3.12/site-packages (from pydantic>=2.9.2->albumentations->-r requirements.txt (line 9)) (0.4.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 7)) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/jms/.local/lib/python3.12/site-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 4)) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T11:27:16.077527Z",
     "iopub.status.busy": "2025-10-13T11:27:16.077295Z",
     "iopub.status.idle": "2025-10-13T11:27:22.548576Z",
     "shell.execute_reply": "2025-10-13T11:27:22.548015Z",
     "shell.execute_reply.started": "2025-10-13T11:27:16.077501Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from sklearn.metrics import jaccard_score\n",
    "import numpy as np\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import time\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import datetime\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 기본 디렉토리 설정 \n",
    "# TRAIN_DIR = \"/kaggle/input/2025-sw-ai/archive/train\"\n",
    "# VAL_DIR = \"/kaggle/input/2025-sw-ai/archive/val\"\n",
    "# TEST_DIR = \"/kaggle/input/2025-sw-ai/archive/test/images\"\n",
    "# OUTPUT_PATH = \"/kaggle/working/submission.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로컬 디렉토리 설정\n",
    "TRAIN_DIR = \"input/2025-csu-sw-ai-challenge/archive/train\" \n",
    "VAL_DIR = \"input/2025-csu-sw-ai-challenge/archive/val\"\n",
    "TEST_DIR = \"input/2025-csu-sw-ai-challenge/archive/test/images\"\n",
    "OUTPUT_CSV = \"working/submission.csv\" \n",
    "OUTPUT_MASK = \"working/mask_ouputs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set SEED: 2025\n"
     ]
    }
   ],
   "source": [
    "SEED = 2025\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f'set SEED: {SEED}')\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T11:27:22.549403Z",
     "iopub.status.busy": "2025-10-13T11:27:22.549145Z",
     "iopub.status.idle": "2025-10-13T11:27:22.555503Z",
     "shell.execute_reply": "2025-10-13T11:27:22.554811Z",
     "shell.execute_reply.started": "2025-10-13T11:27:22.549387Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jms/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from albumentations import (\n",
    "    Compose, HorizontalFlip, VerticalFlip, RandomRotate90, ShiftScaleRotate,\n",
    "    RandomBrightnessContrast, GaussNoise, OneOf, Blur, MotionBlur, RandomGamma\n",
    ")\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "def get_transform():\n",
    "    return Compose([\n",
    "        HorizontalFlip(p=0.5),\n",
    "        VerticalFlip(p=0.5),\n",
    "        RandomRotate90(p=0.5),\n",
    "        ShiftScaleRotate(shift_limit=0.06, scale_limit=0.15, rotate_limit=45, p=0.5),\n",
    "        OneOf([\n",
    "            GaussNoise(variance=(10.0, 40.0)),\n",
    "            Blur(blur_limit=3),\n",
    "            MotionBlur(blur_limit=3)\n",
    "        ], p=0.5),\n",
    "        RandomBrightnessContrast(p=0.4),\n",
    "        RandomGamma(p=0.3),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "class CrackDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, augment_ratio=1):\n",
    "        self.img_dir = os.path.join(root_dir, \"images\")\n",
    "        self.mask_dir = os.path.join(root_dir, \"masks\")\n",
    "        self.img_list = sorted(glob.glob(self.img_dir + \"/*.jpg\"))\n",
    "        self.mask_list = sorted(glob.glob(self.mask_dir + \"/*.jpg\"))\n",
    "        self.transform = transform\n",
    "        self.augment_ratio = augment_ratio  # 추가!\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list) * self.augment_ratio  # 원본x배수\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        orig_idx = idx // self.augment_ratio  # 원본 인덱스 재설정\n",
    "        img = Image.open(self.img_list[orig_idx]).convert(\"L\")\n",
    "        mask = Image.open(self.mask_list[orig_idx]).convert(\"L\")\n",
    "\n",
    "        img = np.array(img, dtype=np.float32) / 255.0\n",
    "        mask = np.array(mask, dtype=np.float32) / 255.0\n",
    "        mask = (mask > 0.5).astype(np.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=img, mask=mask)\n",
    "            img = augmented['image']  # (1,H,W) tensor\n",
    "            mask = augmented['mask'].unsqueeze(0).float()\n",
    "        else:\n",
    "            img = torch.tensor(img).unsqueeze(0)\n",
    "            mask = torch.tensor(mask).unsqueeze(0)\n",
    "\n",
    "        return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T11:27:22.557199Z",
     "iopub.status.busy": "2025-10-13T11:27:22.556966Z",
     "iopub.status.idle": "2025-10-13T11:27:22.580819Z",
     "shell.execute_reply": "2025-10-13T11:27:22.580155Z",
     "shell.execute_reply.started": "2025-10-13T11:27:22.557184Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class HrSegNetB16(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels=1,  # input channel\n",
    "                 base=16,  # base channel of the model, \n",
    "                 num_classes=1,  # number of classes\n",
    "                 pretrained=None  # pretrained model\n",
    "                 ):\n",
    "        super(HrSegNetB16, self).__init__()\n",
    "        self.base = base\n",
    "        self.num_classes = num_classes\n",
    "        self.pretrained = pretrained\n",
    "        # Stage 1 and 2 constitute the stem of the model, which is mainly used to extract low-level features.\n",
    "        # Meanwhile, stage1 and 2 reduce the input image to 1/2 and 1/4 of the original size respectively\n",
    "        self.stage1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=base // 2, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(base // 2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.stage2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=base // 2, out_channels=base, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(base),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.seg1 = SegBlock(base=base, stage_index=1)\n",
    "        self.seg2 = SegBlock(base=base, stage_index=2)\n",
    "        self.seg3 = SegBlock(base=base, stage_index=3)\n",
    "\n",
    "        self.aux_head1 = SegHead(inplanes=base, interplanes=base, outplanes=num_classes, aux_head=True)\n",
    "        self.aux_head2 = SegHead(inplanes=base, interplanes=base, outplanes=num_classes, aux_head=True)\n",
    "        self.head = SegHead(inplanes=base, interplanes=base, outplanes=num_classes)\n",
    "\n",
    "        self.init_weight()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        logit_list = []\n",
    "        h, w = x.shape[2:]\n",
    "        # aux_head only used in training\n",
    "        if self.training:\n",
    "            stem1_out = self.stage1(x)\n",
    "            stem2_out = self.stage2(stem1_out)\n",
    "            hrseg1_out = self.seg1(stem2_out)\n",
    "            hrseg2_out = self.seg2(hrseg1_out)\n",
    "            hrseg3_out = self.seg3(hrseg2_out)\n",
    "            last_out = self.head(hrseg3_out)\n",
    "            seghead1_out = self.aux_head1(hrseg1_out)\n",
    "            seghead2_out = self.aux_head2(hrseg2_out)\n",
    "            logit_list = [last_out, seghead1_out, seghead2_out]\n",
    "            logit_list = [F.interpolate(logit, size=(h, w), mode='bilinear', align_corners=True) for logit in logit_list]\n",
    "            return logit_list\n",
    "        else:\n",
    "            stem1_out = self.stage1(x)\n",
    "            stem2_out = self.stage2(stem1_out)\n",
    "            hrseg1_out = self.seg1(stem2_out)\n",
    "            hrseg2_out = self.seg2(hrseg1_out)\n",
    "            hrseg3_out = self.seg3(hrseg2_out)\n",
    "            last_out = self.head(hrseg3_out)\n",
    "            logit_list = [last_out]\n",
    "            logit_list = [F.interpolate(logit, size=(h, w), mode='bilinear', align_corners=True) for logit in logit_list]\n",
    "            return logit_list\n",
    "        \n",
    "    \n",
    "    def init_weight(self):\n",
    "        if self.pretrained is not None:\n",
    "            pass\n",
    "        else:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Conv2d):\n",
    "                    nn.init.kaiming_normal_(m.weight)\n",
    "                elif isinstance(m, nn.BatchNorm2d):\n",
    "                    nn.init.constant_(m.weight, 1)\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "class SegBlock(nn.Module):\n",
    "    def __init__(self, base=32, stage_index=1):\n",
    "        super(SegBlock, self).__init__()\n",
    "\n",
    "        # Convolutional layer for high-resolution paths with constant spatial resolution and constant channel\n",
    "        self.h_conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=base, out_channels=base, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(base),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.h_conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=base, out_channels=base, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(base),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.h_conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=base, out_channels=base, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(base),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Semantic guidance path/low-resolution path\n",
    "        if stage_index == 1:  # First stage, stride=2, spatial resolution/2, channel*2\n",
    "            self.l_conv1 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=base, out_channels=base * int(math.pow(2, stage_index)), kernel_size=3, stride=2, padding=1),\n",
    "                nn.BatchNorm2d(base * int(math.pow(2, stage_index))),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        elif stage_index == 2:  # Second stage\n",
    "            self.l_conv1 = nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=3, stride=2, padding=1),\n",
    "                nn.Conv2d(in_channels=base, out_channels=base * int(math.pow(2, stage_index)), kernel_size=3, stride=2, padding=1),\n",
    "                nn.BatchNorm2d(base * int(math.pow(2, stage_index))),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        elif stage_index == 3:\n",
    "            self.l_conv1 = nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=3, stride=2, padding=1),\n",
    "                nn.Conv2d(in_channels=base, out_channels=base * int(math.pow(2, stage_index)), kernel_size=3, stride=2, padding=1),\n",
    "                nn.BatchNorm2d(base * int(math.pow(2, stage_index))),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=base * int(math.pow(2, stage_index)), out_channels=base * int(math.pow(2, stage_index)), kernel_size=3, stride=2, padding=1),\n",
    "                nn.BatchNorm2d(base * int(math.pow(2, stage_index))),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"stage_index must be 1, 2 or 3\")\n",
    "        self.l_conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=base * int(math.pow(2, stage_index)), out_channels=base * int(math.pow(2, stage_index)), kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(base * int(math.pow(2, stage_index))),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.l_conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=base * int(math.pow(2, stage_index)), out_channels=base * int(math.pow(2, stage_index)), kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(base * int(math.pow(2, stage_index))),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.l2h_conv1 = nn.Conv2d(in_channels=base * int(math.pow(2, stage_index)), out_channels=base, kernel_size=1, stride=1, padding=0)\n",
    "        self.l2h_conv2 = nn.Conv2d(in_channels=base * int(math.pow(2, stage_index)), out_channels=base, kernel_size=1, stride=1, padding=0)\n",
    "        self.l2h_conv3 = nn.Conv2d(in_channels=base * int(math.pow(2, stage_index)), out_channels=base, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        size = x.shape[2:]\n",
    "        out_h1 = self.h_conv1(x)  # high resolution path\n",
    "        out_l1 = self.l_conv1(x)  # low resolution path\n",
    "        out_l1_i = F.interpolate(out_l1, size=size, mode='bilinear', align_corners=True)  # upsample\n",
    "        out_hl1 = self.l2h_conv1(out_l1_i) + out_h1  # low to high\n",
    "\n",
    "        out_h2 = self.h_conv2(out_hl1)\n",
    "        out_l2 = self.l_conv2(out_l1)\n",
    "        out_l2_i = F.interpolate(out_l2, size=size, mode='bilinear', align_corners=True)\n",
    "        out_hl2 = self.l2h_conv2(out_l2_i) + out_h2\n",
    "\n",
    "        out_h3 = self.h_conv3(out_hl2)\n",
    "        out_l3 = self.l_conv3(out_l2)\n",
    "        out_l3_i = F.interpolate(out_l3, size=size, mode='bilinear', align_corners=True)\n",
    "        out_hl3 = self.l2h_conv3(out_l3_i) + out_h3\n",
    "        return out_hl3\n",
    "\n",
    "# seg head\n",
    "class SegHead(nn.Module):\n",
    "    def __init__(self, inplanes, interplanes, outplanes, aux_head=False):\n",
    "        super(SegHead, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(inplanes)\n",
    "        self.relu = nn.ReLU()\n",
    "        if aux_head:\n",
    "            self.con_bn_relu = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=inplanes, out_channels=interplanes, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(interplanes),\n",
    "                nn.ReLU(),\n",
    "            )\n",
    "        else:\n",
    "            self.con_bn_relu = nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_channels=inplanes, out_channels=interplanes, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                nn.BatchNorm2d(interplanes),\n",
    "                nn.ReLU(),\n",
    "            )\n",
    "        self.conv = nn.Conv2d(in_channels=interplanes, out_channels=outplanes, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.con_bn_relu(x)\n",
    "        out = self.conv(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T11:27:22.581758Z",
     "iopub.status.busy": "2025-10-13T11:27:22.581581Z",
     "iopub.status.idle": "2025-10-13T11:27:22.596935Z",
     "shell.execute_reply": "2025-10-13T11:27:22.596301Z",
     "shell.execute_reply.started": "2025-10-13T11:27:22.581742Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def binary_metrics(preds, targets, eps=1e-6):\n",
    "    preds = preds.float()\n",
    "    targets = targets.float()\n",
    "\n",
    "    tp = (preds * targets).sum(dim=(1,2,3))\n",
    "    fp = (preds * (1 - targets)).sum(dim=(1,2,3))\n",
    "    fn = ((1 - preds) * targets).sum(dim=(1,2,3))\n",
    "\n",
    "    precision = (tp + eps) / (tp + fp + eps)\n",
    "    recall    = (tp + eps) / (tp + fn + eps)\n",
    "    f1        = (2 * precision * recall + eps) / (precision + recall + eps)  # Dice\n",
    "    union     = tp + fp + fn\n",
    "    iou       = (tp + eps) / (union + eps)\n",
    "\n",
    "    return {\n",
    "        \"iou\": iou.mean().item(),\n",
    "        \"precision\": precision.mean().item(),\n",
    "        \"recall\": recall.mean().item(),\n",
    "        \"f1\": f1.mean().item(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# --- [추가] F-beta 손실 함수 정의 ---\n",
    "class FbetaLoss(nn.Module):\n",
    "    def __init__(self, beta=2.0, smooth=1e-6):\n",
    "        super(FbetaLoss, self).__init__()\n",
    "        self.beta = beta\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        preds = torch.sigmoid(logits)\n",
    "\n",
    "        tp = (preds * targets).sum(dim=(2, 3))\n",
    "        fp = (preds * (1 - targets)).sum(dim=(2, 3))\n",
    "        fn = ((1 - preds) * targets).sum(dim=(2, 3))\n",
    "\n",
    "        beta2 = self.beta ** 2\n",
    "        f_beta = ((1 + beta2) * tp + self.smooth) / ((1 + beta2) * tp + beta2 * fn + fp + self.smooth)\n",
    "        \n",
    "        return 1 - f_beta.mean()\n",
    "\n",
    "# --- [추가] F-beta 점수 계산 함수 (검증용) ---\n",
    "def calculate_fbeta(preds, masks, beta=2.0, smooth=1e-6):\n",
    "    preds = preds.float()\n",
    "    masks = masks.float()\n",
    "\n",
    "    # 배치 전체를 하나의 큰 이미지로 보고 계산\n",
    "    tp = (preds * masks).sum()\n",
    "    fp = (preds * (1 - masks)).sum()\n",
    "    fn = ((1 - preds) * masks).sum()\n",
    "    \n",
    "    beta2 = beta ** 2\n",
    "    f_beta = ((1 + beta2) * tp + smooth) / ((1 + beta2) * tp + beta2 * fn + fp + smooth)\n",
    "    \n",
    "    return f_beta.item()\n",
    "\n",
    "# --- [적용] 사용자께서 제공하신 binary_metrics 함수 ---\n",
    "def binary_metrics(preds, targets, eps=1e-6):\n",
    "    preds = preds.float()\n",
    "    targets = targets.float()\n",
    "\n",
    "    # 배치 내 각 샘플에 대해 TP, FP, FN 계산\n",
    "    tp = (preds * targets).sum(dim=(1,2,3))\n",
    "    fp = (preds * (1 - targets)).sum(dim=(1,2,3))\n",
    "    fn = ((1 - preds) * targets).sum(dim=(1,2,3))\n",
    "\n",
    "    precision = (tp + eps) / (tp + fp + eps)\n",
    "    recall    = (tp + eps) / (tp + fn + eps)\n",
    "    f1        = (2 * precision * recall + eps) / (precision + recall + eps)\n",
    "    union     = tp + fp + fn\n",
    "    iou       = (tp + eps) / (union + eps)\n",
    "\n",
    "    # 각 지표를 배치에 대해 평균내어 반환\n",
    "    return {\n",
    "        \"iou\": iou.mean().item(),\n",
    "        \"precision\": precision.mean().item(),\n",
    "        \"recall\": recall.mean().item(),\n",
    "        \"f1\": f1.mean().item(),\n",
    "    }\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# train_model 함수\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    device,\n",
    "    epochs=10,\n",
    "    aux_weights=(1.0, 0.4, 0.4),\n",
    "    lr=1e-3,\n",
    "    use_amp=False,\n",
    "    log_every=500,\n",
    "    validate_every_steps=None,\n",
    "    threshold=0.5,\n",
    "    patience=5,\n",
    "    model_save_path='best_model.pth'\n",
    "):\n",
    "    model.to(device)\n",
    "    criterion = FbetaLoss(beta=2.0)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "    # --- [추가] Early Stopping을 위한 변수 초기화 ---\n",
    "    patience_counter = 0\n",
    "    best_val_fbeta = 0.0\n",
    "\n",
    "    global_step = 0\n",
    "    # --- [복원] Step 단위 로깅을 위한 변수 ---\n",
    "    win_loss, win_iou, win_f1, win_steps = 0.0, 0.0, 0.0, 0\n",
    "    t0 = time.time()\n",
    "\n",
    "    best_threshold = threshold  # F2 score 최적화 위해 threshold 저장 변수 추가\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for imgs, masks in train_loader:\n",
    "            global_step += 1\n",
    "            imgs = imgs.to(device, non_blocking=True)\n",
    "            masks = masks.to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "                outputs = model(imgs)\n",
    "                main_logit = outputs[0] if isinstance(outputs, (list, tuple)) else outputs\n",
    "                \n",
    "                loss = aux_weights[0] * criterion(main_logit, masks)\n",
    "                if isinstance(outputs, (list, tuple)):\n",
    "                    if len(outputs) > 1 and aux_weights[1] > 0:\n",
    "                        loss = loss + aux_weights[1] * criterion(outputs[1], masks)\n",
    "                    if len(outputs) > 2 and aux_weights[2] > 0:\n",
    "                        loss = loss + aux_weights[2] * criterion(outputs[2], masks)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # --- [복원] Step 단위 로깅을 위한 집계 로직 ---\n",
    "            win_loss += loss.item()\n",
    "            win_steps += 1\n",
    "            with torch.no_grad():\n",
    "                probs = torch.sigmoid(main_logit)\n",
    "                preds = (probs > best_threshold).float()\n",
    "                m = binary_metrics(preds, masks)\n",
    "                win_iou += m[\"iou\"]\n",
    "                win_f1  += m[\"f1\"]\n",
    "            \n",
    "            # --- [복원] Step 단위 로깅 출력문 ---\n",
    "            if log_every and (global_step % log_every == 0):\n",
    "                elapsed = time.time() - t0\n",
    "                lr_now = optimizer.param_groups[0][\"lr\"]\n",
    "                print(f\"[Step {global_step}] epoch={epoch}  \"\n",
    "                      f\"avg_loss(win)={win_loss/max(1,win_steps):.4f}  \"\n",
    "                      f\"avg_iou(win)={win_iou/max(1,win_steps):.4f}  \"\n",
    "                      f\"avg_f1(win)={win_f1/max(1,win_steps):.4f}  \"\n",
    "                      f\"lr={lr_now:.3e}  elapsed={elapsed:.1f}s\")\n",
    "                win_loss = win_iou = win_f1 = 0.0\n",
    "                win_steps = 0\n",
    "                t0 = time.time()\n",
    "\n",
    "        # --- Epoch 종료 후 검증 ---\n",
    "        avg_train_loss = epoch_loss / max(1, len(train_loader))\n",
    "        model.eval()\n",
    "\n",
    "        val_fbeta_list, val_loss_list, val_iou_list, val_f1_list = [], [], [], []\n",
    "\n",
    "        all_probs = []\n",
    "        all_masks = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for imgs, masks in val_loader:\n",
    "                imgs = imgs.to(device, non_blocking=True)\n",
    "                masks = masks.to(device, non_blocking=True)\n",
    "\n",
    "                logits_list = model(imgs)\n",
    "                main_logit = logits_list[0] if isinstance(logits_list, (list, tuple)) else logits_list\n",
    "\n",
    "                loss = criterion(main_logit, masks)\n",
    "                val_loss_list.append(loss.item())\n",
    "\n",
    "                probs = torch.sigmoid(main_logit)\n",
    "                all_probs.append(probs.cpu())\n",
    "                all_masks.append(masks.cpu())\n",
    "\n",
    "                preds = (probs > best_threshold).float()\n",
    "\n",
    "                fbeta_score = calculate_fbeta(preds, masks, beta=2.0)\n",
    "                val_fbeta_list.append(fbeta_score)\n",
    "\n",
    "                m = binary_metrics(preds, masks)\n",
    "                val_iou_list.append(m[\"iou\"])\n",
    "                val_f1_list.append(m[\"f1\"])\n",
    "\n",
    "        all_probs = torch.cat(all_probs, dim=0).numpy()\n",
    "        all_masks = torch.cat(all_masks, dim=0).numpy()\n",
    "\n",
    "        # --- Threshold 최적화 (F2 score 기준) ---\n",
    "        thresholds = np.linspace(0.01, 0.99, 99)\n",
    "        best_score = 0.0\n",
    "        best_epoch_threshold = best_threshold\n",
    "\n",
    "        for t in thresholds:\n",
    "            preds_bin = (all_probs > t).astype(np.uint8)\n",
    "            tp = (preds_bin * all_masks).sum(axis=(1,2,3))\n",
    "            fp = (preds_bin * (1 - all_masks)).sum(axis=(1,2,3))\n",
    "            fn = ((1 - preds_bin) * all_masks).sum(axis=(1,2,3))\n",
    "\n",
    "            beta2 = 2.0 ** 2\n",
    "            f_beta = ((1 + beta2) * tp + 1e-6) / ((1 + beta2) * tp + beta2 * fn + fp + 1e-6)\n",
    "            score = f_beta.mean()\n",
    "\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_epoch_threshold = t\n",
    "        # --- Threshold 최적화 끝 ---\n",
    "\n",
    "        avg_val_loss = np.mean(val_loss_list)\n",
    "        avg_val_fbeta = np.mean(val_fbeta_list)\n",
    "        avg_val_iou = np.mean(val_iou_list)\n",
    "        avg_val_f1 = np.mean(val_f1_list)\n",
    "\n",
    "        # --- [복원] Epoch 단위 출력문에 F2 score, 최적 threshold 출력 추가 ---\n",
    "        print(f\"[Epoch {epoch}/{epochs}] \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "              f\"Val Loss: {avg_val_loss:.4f} | \"\n",
    "              f\"Val IoU: {avg_val_iou:.4f} | \"\n",
    "              f\"Val F1: {avg_val_f1:.4f} | \"\n",
    "              f\"Val F-beta(β=2.0): {avg_val_fbeta:.4f} | \"\n",
    "              f\"Best Threshold: {best_epoch_threshold:.3f} | \"\n",
    "              f\"Best F-beta@threshold: {best_score:.4f}\")\n",
    "\n",
    "        # Early Stopping 로직 (F-beta 기준)\n",
    "        if best_score > best_val_fbeta:\n",
    "            best_val_fbeta = best_score\n",
    "            best_threshold = best_epoch_threshold\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f\" -> Best score updated. Model saved.\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\" -> Patience: {patience_counter}/{patience}\")\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping triggered after {patience} epochs without improvement.\")\n",
    "            break\n",
    "            \n",
    "    print(f\"\\nTraining finished. Best Val F-beta(β=2.0) was: {best_val_fbeta:.4f}\")\n",
    "    print(f\"Best Threshold found: {best_threshold:.3f}\")\n",
    "\n",
    "    model.load_state_dict(torch.load(model_save_path))\n",
    "    return model, best_threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T11:27:22.597755Z",
     "iopub.status.busy": "2025-10-13T11:27:22.597539Z",
     "iopub.status.idle": "2025-10-13T11:27:22.853695Z",
     "shell.execute_reply": "2025-10-13T11:27:22.853224Z",
     "shell.execute_reply.started": "2025-10-13T11:27:22.597730Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load..tarin data\n",
      "Loaded train data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jms/.local/lib/python3.12/site-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n",
      "/tmp/ipykernel_2128/401983115.py:15: UserWarning: Argument(s) 'variance' are not valid for transform GaussNoise\n",
      "  GaussNoise(variance=(10.0, 40.0)),\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CrackDataset(TRAIN_DIR, transform=get_transform(), augment_ratio=2)\n",
    "val_dataset = CrackDataset(VAL_DIR, transform=None, augment_ratio=1)\n",
    "\n",
    "print(\"Load..tarin data\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "print(\"Loaded train data\")\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T11:27:22.854526Z",
     "iopub.status.busy": "2025-10-13T11:27:22.854362Z",
     "iopub.status.idle": "2025-10-13T11:27:27.325732Z",
     "shell.execute_reply": "2025-10-13T11:27:27.324895Z",
     "shell.execute_reply.started": "2025-10-13T11:27:22.854513Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params: 609.75 k\n",
      "Total MACs: 130.86 MMac\n"
     ]
    }
   ],
   "source": [
    "from ptflops import get_model_complexity_info\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HrSegNetB16().to(device)\n",
    "input_size = (1, 192, 192)\n",
    "\n",
    "macs, params = get_model_complexity_info(model,\n",
    "                                         input_size,\n",
    "                                         as_strings=True,\n",
    "                                         print_per_layer_stat=False,\n",
    "                                         verbose=False)\n",
    "print(f\"Total Params: {params}\")\n",
    "print(f\"Total MACs: {macs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T11:27:27.326740Z",
     "iopub.status.busy": "2025-10-13T11:27:27.326532Z",
     "iopub.status.idle": "2025-10-13T11:40:59.805475Z",
     "shell.execute_reply": "2025-10-13T11:40:59.804776Z",
     "shell.execute_reply.started": "2025-10-13T11:27:27.326723Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2128/4240423271.py:87: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
      "/tmp/ipykernel_2128/4240423271.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 500] epoch=1  avg_loss(win)=1.4402  avg_iou(win)=0.0837  avg_f1(win)=0.1594  lr=1.000e-03  elapsed=16.7s\n",
      "[Step 1000] epoch=1  avg_loss(win)=1.2227  avg_iou(win)=0.1691  avg_f1(win)=0.2842  lr=1.000e-03  elapsed=14.9s\n",
      "[Step 1500] epoch=1  avg_loss(win)=1.1413  avg_iou(win)=0.2078  avg_f1(win)=0.3282  lr=1.000e-03  elapsed=14.9s\n",
      "[Epoch 1/40] Train Loss: 1.2418 | Val Loss: 0.5329 | Val IoU: 0.2877 | Val F1: 0.4206 | Val F-beta(β=2.0): 0.5716 | Best Threshold: 0.090 | Best F-beta@threshold: 0.4715\n",
      " -> Best score updated. Model saved.\n",
      "[Step 2000] epoch=2  avg_loss(win)=1.1372  avg_iou(win)=0.2073  avg_f1(win)=0.3312  lr=1.000e-03  elapsed=32.1s\n",
      "[Step 2500] epoch=2  avg_loss(win)=1.0896  avg_iou(win)=0.2224  avg_f1(win)=0.3418  lr=1.000e-03  elapsed=13.6s\n",
      "[Step 3000] epoch=2  avg_loss(win)=1.0804  avg_iou(win)=0.2286  avg_f1(win)=0.3507  lr=1.000e-03  elapsed=13.6s\n",
      "[Step 3500] epoch=2  avg_loss(win)=1.0511  avg_iou(win)=0.2423  avg_f1(win)=0.3691  lr=1.000e-03  elapsed=12.8s\n",
      "[Epoch 2/40] Train Loss: 1.0745 | Val Loss: 0.4843 | Val IoU: 0.3082 | Val F1: 0.4373 | Val F-beta(β=2.0): 0.6356 | Best Threshold: 0.850 | Best F-beta@threshold: 0.5203\n",
      " -> Best score updated. Model saved.\n",
      "[Step 4000] epoch=3  avg_loss(win)=1.0494  avg_iou(win)=0.2490  avg_f1(win)=0.3715  lr=1.000e-03  elapsed=31.6s\n",
      "[Step 4500] epoch=3  avg_loss(win)=1.0552  avg_iou(win)=0.2563  avg_f1(win)=0.3860  lr=1.000e-03  elapsed=14.2s\n",
      "[Step 5000] epoch=3  avg_loss(win)=1.0353  avg_iou(win)=0.2644  avg_f1(win)=0.3948  lr=1.000e-03  elapsed=13.9s\n",
      "[Step 5500] epoch=3  avg_loss(win)=1.0177  avg_iou(win)=0.2692  avg_f1(win)=0.4008  lr=1.000e-03  elapsed=13.8s\n",
      "[Epoch 3/40] Train Loss: 1.0333 | Val Loss: 0.4968 | Val IoU: 0.3123 | Val F1: 0.4471 | Val F-beta(β=2.0): 0.6264 | Best Threshold: 0.250 | Best F-beta@threshold: 0.5047\n",
      " -> Patience: 1/5\n",
      "[Step 6000] epoch=4  avg_loss(win)=1.0295  avg_iou(win)=0.2647  avg_f1(win)=0.3931  lr=1.000e-03  elapsed=32.2s\n",
      "[Step 6500] epoch=4  avg_loss(win)=1.0067  avg_iou(win)=0.2753  avg_f1(win)=0.4075  lr=1.000e-03  elapsed=13.7s\n",
      "[Step 7000] epoch=4  avg_loss(win)=1.0233  avg_iou(win)=0.2674  avg_f1(win)=0.3925  lr=1.000e-03  elapsed=13.1s\n",
      "[Step 7500] epoch=4  avg_loss(win)=1.0125  avg_iou(win)=0.2717  avg_f1(win)=0.4004  lr=1.000e-03  elapsed=13.3s\n",
      "[Epoch 4/40] Train Loss: 1.0199 | Val Loss: 0.4740 | Val IoU: 0.3208 | Val F1: 0.4517 | Val F-beta(β=2.0): 0.6463 | Best Threshold: 0.990 | Best F-beta@threshold: 0.5335\n",
      " -> Best score updated. Model saved.\n",
      "[Step 8000] epoch=5  avg_loss(win)=1.0135  avg_iou(win)=0.2815  avg_f1(win)=0.4154  lr=1.000e-03  elapsed=30.5s\n",
      "[Step 8500] epoch=5  avg_loss(win)=1.0062  avg_iou(win)=0.2826  avg_f1(win)=0.4126  lr=1.000e-03  elapsed=13.4s\n",
      "[Step 9000] epoch=5  avg_loss(win)=0.9842  avg_iou(win)=0.2919  avg_f1(win)=0.4230  lr=1.000e-03  elapsed=13.0s\n",
      "[Epoch 5/40] Train Loss: 0.9973 | Val Loss: 0.4777 | Val IoU: 0.3277 | Val F1: 0.4528 | Val F-beta(β=2.0): 0.6407 | Best Threshold: 0.990 | Best F-beta@threshold: 0.5282\n",
      " -> Patience: 1/5\n",
      "[Step 9500] epoch=6  avg_loss(win)=0.9834  avg_iou(win)=0.2956  avg_f1(win)=0.4274  lr=1.000e-03  elapsed=30.9s\n",
      "[Step 10000] epoch=6  avg_loss(win)=1.0054  avg_iou(win)=0.2917  avg_f1(win)=0.4238  lr=1.000e-03  elapsed=13.3s\n",
      "[Step 10500] epoch=6  avg_loss(win)=0.9834  avg_iou(win)=0.2972  avg_f1(win)=0.4303  lr=1.000e-03  elapsed=13.4s\n",
      "[Step 11000] epoch=6  avg_loss(win)=0.9974  avg_iou(win)=0.2885  avg_f1(win)=0.4200  lr=1.000e-03  elapsed=13.9s\n",
      "[Epoch 6/40] Train Loss: 0.9928 | Val Loss: 0.4631 | Val IoU: 0.3460 | Val F1: 0.4746 | Val F-beta(β=2.0): 0.6557 | Best Threshold: 0.990 | Best F-beta@threshold: 0.5388\n",
      " -> Best score updated. Model saved.\n",
      "[Step 11500] epoch=7  avg_loss(win)=0.9907  avg_iou(win)=0.2877  avg_f1(win)=0.4189  lr=1.000e-03  elapsed=34.2s\n",
      "[Step 12000] epoch=7  avg_loss(win)=0.9910  avg_iou(win)=0.2870  avg_f1(win)=0.4175  lr=1.000e-03  elapsed=13.8s\n",
      "[Step 12500] epoch=7  avg_loss(win)=0.9692  avg_iou(win)=0.2968  avg_f1(win)=0.4286  lr=1.000e-03  elapsed=13.5s\n",
      "[Step 13000] epoch=7  avg_loss(win)=0.9772  avg_iou(win)=0.2961  avg_f1(win)=0.4308  lr=1.000e-03  elapsed=13.5s\n",
      "[Epoch 7/40] Train Loss: 0.9874 | Val Loss: 0.5530 | Val IoU: 0.3438 | Val F1: 0.4408 | Val F-beta(β=2.0): 0.4321 | Best Threshold: 0.010 | Best F-beta@threshold: 0.4560\n",
      " -> Patience: 1/5\n",
      "[Step 13500] epoch=8  avg_loss(win)=1.0465  avg_iou(win)=0.3046  avg_f1(win)=0.4366  lr=1.000e-03  elapsed=31.8s\n",
      "[Step 14000] epoch=8  avg_loss(win)=1.1053  avg_iou(win)=0.2798  avg_f1(win)=0.4164  lr=1.000e-03  elapsed=14.1s\n",
      "[Step 14500] epoch=8  avg_loss(win)=1.0309  avg_iou(win)=0.3130  avg_f1(win)=0.4381  lr=1.000e-03  elapsed=14.4s\n",
      "[Step 15000] epoch=8  avg_loss(win)=1.0223  avg_iou(win)=0.3223  avg_f1(win)=0.4410  lr=1.000e-03  elapsed=15.6s\n",
      "[Epoch 8/40] Train Loss: 1.0497 | Val Loss: 0.4006 | Val IoU: 0.4333 | Val F1: 0.5519 | Val F-beta(β=2.0): 0.6072 | Best Threshold: 0.020 | Best F-beta@threshold: 0.6039\n",
      " -> Best score updated. Model saved.\n",
      "[Step 15500] epoch=9  avg_loss(win)=0.9767  avg_iou(win)=0.3303  avg_f1(win)=0.4520  lr=1.000e-03  elapsed=34.6s\n",
      "[Step 16000] epoch=9  avg_loss(win)=1.0198  avg_iou(win)=0.3096  avg_f1(win)=0.4255  lr=1.000e-03  elapsed=15.1s\n",
      "[Step 16500] epoch=9  avg_loss(win)=0.9550  avg_iou(win)=0.3398  avg_f1(win)=0.4562  lr=1.000e-03  elapsed=14.9s\n",
      "[Epoch 9/40] Train Loss: 0.9712 | Val Loss: 0.3813 | Val IoU: 0.4313 | Val F1: 0.5530 | Val F-beta(β=2.0): 0.6487 | Best Threshold: 0.030 | Best F-beta@threshold: 0.6217\n",
      " -> Best score updated. Model saved.\n",
      "[Step 17000] epoch=10  avg_loss(win)=0.9360  avg_iou(win)=0.3347  avg_f1(win)=0.4558  lr=1.000e-03  elapsed=33.8s\n",
      "[Step 17500] epoch=10  avg_loss(win)=0.9560  avg_iou(win)=0.3259  avg_f1(win)=0.4488  lr=1.000e-03  elapsed=13.9s\n",
      "[Step 18000] epoch=10  avg_loss(win)=1.0017  avg_iou(win)=0.3149  avg_f1(win)=0.4372  lr=1.000e-03  elapsed=14.4s\n",
      "[Step 18500] epoch=10  avg_loss(win)=1.0170  avg_iou(win)=0.3132  avg_f1(win)=0.4286  lr=1.000e-03  elapsed=14.2s\n",
      "[Epoch 10/40] Train Loss: 0.9931 | Val Loss: 0.4117 | Val IoU: 0.4100 | Val F1: 0.5277 | Val F-beta(β=2.0): 0.6288 | Best Threshold: 0.050 | Best F-beta@threshold: 0.5916\n",
      " -> Patience: 1/5\n",
      "[Step 19000] epoch=11  avg_loss(win)=1.0006  avg_iou(win)=0.3149  avg_f1(win)=0.4353  lr=1.000e-03  elapsed=31.2s\n",
      "[Step 19500] epoch=11  avg_loss(win)=1.0070  avg_iou(win)=0.3116  avg_f1(win)=0.4312  lr=1.000e-03  elapsed=13.6s\n",
      "[Step 20000] epoch=11  avg_loss(win)=1.0002  avg_iou(win)=0.3194  avg_f1(win)=0.4379  lr=1.000e-03  elapsed=13.8s\n",
      "[Step 20500] epoch=11  avg_loss(win)=0.9822  avg_iou(win)=0.3305  avg_f1(win)=0.4477  lr=1.000e-03  elapsed=13.4s\n",
      "[Epoch 11/40] Train Loss: 0.9979 | Val Loss: 0.4479 | Val IoU: 0.3892 | Val F1: 0.4960 | Val F-beta(β=2.0): 0.5725 | Best Threshold: 0.020 | Best F-beta@threshold: 0.5531\n",
      " -> Patience: 2/5\n",
      "[Step 21000] epoch=12  avg_loss(win)=1.0051  avg_iou(win)=0.3196  avg_f1(win)=0.4323  lr=1.000e-03  elapsed=30.8s\n",
      "[Step 21500] epoch=12  avg_loss(win)=0.9690  avg_iou(win)=0.3243  avg_f1(win)=0.4424  lr=1.000e-03  elapsed=13.3s\n",
      "[Step 22000] epoch=12  avg_loss(win)=0.9181  avg_iou(win)=0.3432  avg_f1(win)=0.4665  lr=1.000e-03  elapsed=13.4s\n",
      "[Step 22500] epoch=12  avg_loss(win)=0.9224  avg_iou(win)=0.3371  avg_f1(win)=0.4678  lr=1.000e-03  elapsed=13.3s\n",
      "[Epoch 12/40] Train Loss: 0.9616 | Val Loss: 0.5051 | Val IoU: 0.3310 | Val F1: 0.4720 | Val F-beta(β=2.0): 0.5050 | Best Threshold: 0.030 | Best F-beta@threshold: 0.4966\n",
      " -> Patience: 3/5\n",
      "[Step 23000] epoch=13  avg_loss(win)=1.0490  avg_iou(win)=0.2862  avg_f1(win)=0.4164  lr=1.000e-03  elapsed=31.8s\n",
      "[Step 23500] epoch=13  avg_loss(win)=0.9812  avg_iou(win)=0.3267  avg_f1(win)=0.4400  lr=1.000e-03  elapsed=14.0s\n",
      "[Step 24000] epoch=13  avg_loss(win)=0.9203  avg_iou(win)=0.3410  avg_f1(win)=0.4637  lr=1.000e-03  elapsed=13.6s\n",
      "[Step 24500] epoch=13  avg_loss(win)=1.1116  avg_iou(win)=0.2774  avg_f1(win)=0.3881  lr=1.000e-03  elapsed=14.1s\n",
      "[Epoch 13/40] Train Loss: 1.0063 | Val Loss: 0.4530 | Val IoU: 0.3973 | Val F1: 0.5062 | Val F-beta(β=2.0): 0.5688 | Best Threshold: 0.020 | Best F-beta@threshold: 0.5486\n",
      " -> Patience: 4/5\n",
      "[Step 25000] epoch=14  avg_loss(win)=1.0173  avg_iou(win)=0.3169  avg_f1(win)=0.4248  lr=1.000e-03  elapsed=31.5s\n",
      "[Step 25500] epoch=14  avg_loss(win)=0.9344  avg_iou(win)=0.3399  avg_f1(win)=0.4589  lr=1.000e-03  elapsed=13.1s\n",
      "[Step 26000] epoch=14  avg_loss(win)=0.9786  avg_iou(win)=0.3228  avg_f1(win)=0.4502  lr=1.000e-03  elapsed=13.6s\n",
      "[Epoch 14/40] Train Loss: 0.9891 | Val Loss: 0.4515 | Val IoU: 0.3581 | Val F1: 0.4677 | Val F-beta(β=2.0): 0.5413 | Best Threshold: 0.990 | Best F-beta@threshold: 0.5514\n",
      " -> Patience: 5/5\n",
      "\n",
      "Early stopping triggered after 5 epochs without improvement.\n",
      "\n",
      "Training finished. Best Val F-beta(β=2.0) was: 0.6217\n",
      "Best Threshold found: 0.030\n"
     ]
    }
   ],
   "source": [
    "print(device)\n",
    "model, best_threshold = train_model(model, train_loader, val_loader, device, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_mask_image(\n",
    "    mask_image: Image.Image,\n",
    "    base_output_dir: str,\n",
    "    original_filename: str,\n",
    "    script_name: str = 'defalut'\n",
    "):\n",
    "    \"\"\"\n",
    "    마스크 이미지를 지정된 규칙에 따라 폴더를 생성하고 저장합니다.\n",
    "\n",
    "    Args:\n",
    "        mask_image (Image.Image): 저장할 PIL 이미지 객체.\n",
    "        base_output_dir (str): 결과 폴더를 생성할 상위 경로.\n",
    "        original_filename (str): 원본 이미지 파일명 (e.g., 'image_001.jpg').\n",
    "        script_name (str): 현재 실행 중인 파이썬 스크립트 또는 노트북 파일명.\n",
    "    \n",
    "    Returns:\n",
    "        str: 파일이 저장된 전체 경로.\n",
    "    \"\"\"\n",
    "    # 1. 'test_파일명_mmddhhmm' 형식으로 폴더명 생성\n",
    "    now = datetime.datetime.now()\n",
    "    timestamp = now.strftime(\"%m%d%H%M\")  # mmddhhmm 형식\n",
    "    \n",
    "    # 스크립트 이름에서 확장자(.py, .ipynb) 제거\n",
    "    script_basename = os.path.splitext(script_name)[0]\n",
    "    \n",
    "    folder_name = f\"test_{script_basename}_{timestamp}\"\n",
    "    output_dir = os.path.join(base_output_dir, folder_name)\n",
    "    \n",
    "    # 폴더 생성 (이미 존재하면 그대로 사용)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # 2. 저장할 파일명 생성 (원본 파일명 기반)\n",
    "    original_basename = os.path.splitext(original_filename)[0]\n",
    "    output_filename = f\"{original_basename}_mask.png\"\n",
    "    \n",
    "    # 3. 전체 저장 경로를 조합하고 이미지 저장\n",
    "    output_path = os.path.join(output_dir, output_filename)\n",
    "    mask_image.save(output_path)\n",
    "    \n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T11:40:59.806608Z",
     "iopub.status.busy": "2025-10-13T11:40:59.806413Z",
     "iopub.status.idle": "2025-10-13T11:41:00.075814Z",
     "shell.execute_reply": "2025-10-13T11:41:00.075230Z",
     "shell.execute_reply.started": "2025-10-13T11:40:59.806593Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def rle_encode(mask):\n",
    "    \"\"\"\n",
    "    mask: 2D numpy array of {0,1}, shape (H,W)\n",
    "    return: run length as string\n",
    "    \"\"\"\n",
    "    pixels = mask.flatten(order=\"C\")\n",
    "    ones = np.where(pixels == 1)[0] + 1  # 1-based\n",
    "    if len(ones) == 0:\n",
    "        return \"\"\n",
    "    runs = []\n",
    "    prev = -2\n",
    "    for idx in ones:\n",
    "        if idx > prev + 1:\n",
    "            runs.extend((idx, 0))\n",
    "        runs[-1] += 1\n",
    "        prev = idx\n",
    "    return \" \".join(map(str, runs))\n",
    "\n",
    "\n",
    "def predict_and_submit(model, test_img_dir, output_csv, device, threshold=0.5):\n",
    "    model.eval()\n",
    "    ids, rles = [], []\n",
    "\n",
    "    test_imgs = sorted(glob.glob(os.path.join(test_img_dir, \"*.jpg\")))\n",
    "    for path in test_imgs:\n",
    "        img_id = os.path.splitext(os.path.basename(path))[0]\n",
    "        img = Image.open(path).convert(\"L\")\n",
    "        arr = np.array(img, dtype=np.float32) / 255.0\n",
    "        tensor = torch.tensor(arr).unsqueeze(0).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out_list = model(tensor)\n",
    "            main_logit = out_list[0] if isinstance(out_list, (list, tuple)) else out_list\n",
    "            prob = torch.sigmoid(main_logit)[0,0].cpu().numpy()\n",
    "            pred = (prob > threshold).astype(np.uint8)\n",
    "        \n",
    "        rle = rle_encode(pred)\n",
    "        ids.append(img_id)\n",
    "        rles.append(rle)\n",
    "\n",
    "    df = pd.DataFrame({\"image_id\": ids, \"rle\": rles})\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"[OK] submission saved to {output_csv}, total {len(df)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rle_encode(mask):\n",
    "    \"\"\"\n",
    "    mask: 2D numpy array of {0,1}, shape (H,W)\n",
    "    return: run length as string\n",
    "    \"\"\"\n",
    "    pixels = mask.flatten(order=\"C\")\n",
    "    ones = np.where(pixels == 1)[0] + 1  # 1-based\n",
    "    if len(ones) == 0:\n",
    "        return \"\"\n",
    "    runs = []\n",
    "    prev = -2\n",
    "    for idx in ones:\n",
    "        if idx > prev + 1:\n",
    "            runs.extend((idx, 0))\n",
    "        runs[-1] += 1\n",
    "        prev = idx\n",
    "    return \" \".join(map(str, runs))\n",
    "\n",
    "\n",
    "def predict_submit_and_save_masks(\n",
    "    model, \n",
    "    test_img_dir, \n",
    "    output_csv, \n",
    "    device, \n",
    "    threshold=0.5,\n",
    "    save_masks=False,\n",
    "    mask_save_dir=None\n",
    "):\n",
    "    \n",
    "    model.eval()\n",
    "    ids, rles = [], []\n",
    "\n",
    "    # --- 이미지 저장을 위한 폴더 설정 ---\n",
    "    output_mask_path = \"\"\n",
    "    if save_masks:\n",
    "        if mask_save_dir is None:\n",
    "            # mask_save_dir가 지정되지 않으면 에러 발생\n",
    "            raise ValueError(\"If save_masks is True, mask_save_dir must be provided.\")\n",
    "        \n",
    "        # 현재 시간을 기반으로 하위 폴더 생성\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_mask_path = os.path.join(mask_save_dir, f\"predictions_{timestamp}\")\n",
    "        os.makedirs(output_mask_path, exist_ok=True)\n",
    "        print(f\"Mask images will be saved to: {output_mask_path}\")\n",
    "\n",
    "    test_imgs = sorted(glob.glob(os.path.join(test_img_dir, \"*.jpg\")))\n",
    "    for path in test_imgs:\n",
    "        img_id = os.path.splitext(os.path.basename(path))[0]\n",
    "        img = Image.open(path).convert(\"L\")\n",
    "        arr = np.array(img, dtype=np.float32) / 255.0\n",
    "        tensor = torch.tensor(arr).unsqueeze(0).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out_list = model(tensor)\n",
    "            main_logit = out_list[0] if isinstance(out_list, (list, tuple)) else out_list\n",
    "            prob = torch.sigmoid(main_logit)[0,0].cpu().numpy()\n",
    "            pred = (prob > threshold).astype(np.uint8)\n",
    "        \n",
    "        # ---  마스크 이미지를 파일로 저장 ---\n",
    "        if save_masks:\n",
    "            mask_image = Image.fromarray(pred * 255, mode='L')\n",
    "            mask_filename = f\"{img_id}_mask.png\"\n",
    "            save_path = os.path.join(output_mask_path, mask_filename)\n",
    "            mask_image.save(save_path)\n",
    "\n",
    "        # --- RLE 인코딩 및 CSV 데이터 수집 ---\n",
    "        rle = rle_encode(pred)\n",
    "        ids.append(img_id)\n",
    "        rles.append(rle)\n",
    "\n",
    "    # --- CSV 파일로 최종 저장 ---\n",
    "    df = pd.DataFrame({\"image_id\": ids, \"rle\": rles})\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"OK. Submission CSV saved to {output_csv}, total {len(df)} rows.\")\n",
    "    \n",
    "    if save_masks:\n",
    "        print(f\"OK. Mask images also saved in: {output_mask_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T11:41:00.077800Z",
     "iopub.status.busy": "2025-10-13T11:41:00.077387Z",
     "iopub.status.idle": "2025-10-13T11:41:33.222667Z",
     "shell.execute_reply": "2025-10-13T11:41:33.222039Z",
     "shell.execute_reply.started": "2025-10-13T11:41:00.077781Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask images will be saved to: working/mask_ouputs/predictions_20251015_205829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2128/4196979983.py:61: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
      "  mask_image = Image.fromarray(pred * 255, mode='L')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK. Submission CSV saved to working/submission.csv, total 2667 rows.\n",
      "OK. Mask images also saved in: working/mask_ouputs/predictions_20251015_205829\n"
     ]
    }
   ],
   "source": [
    "predict_submit_and_save_masks(\n",
    "    model=model,\n",
    "    test_img_dir=TEST_DIR,\n",
    "    output_csv=\"working/submission.csv\",\n",
    "    device=device,\n",
    "    save_masks=True,  \n",
    "    mask_save_dir=OUTPUT_MASK,\n",
    "    threshold = best_threshold,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13948799,
     "sourceId": 115856,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
